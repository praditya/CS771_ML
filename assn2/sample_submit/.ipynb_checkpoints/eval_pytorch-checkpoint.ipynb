{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import predict\n",
    "import time as tm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.datasets import make_classification\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Data\n",
    "dictSize = 225\n",
    "(X, y) = utils.loadData( \"train\", dictSize = dictSize )\n",
    "X = scipy.sparse.csr_matrix.toarray(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/multi-class-classification/47565\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            outp = self.forward(x)\n",
    "            return F.softmax(outp)\n",
    "# add softmax layer: used for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model declared\n",
    "model = NeuralNet(225 , 10000, 51)  \n",
    "# earlier hidden layers- 1000 \n",
    "# 1 - poor, 2- ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "model_opt = optim.SGD(model.parameters(), lr = 0.02)\n",
    "\n",
    "train_set_X = Variable(torch.from_numpy(X_train)).float()\n",
    "train_set_y = Variable(torch.LongTensor(y_train)).long()\n",
    "test_set_X = Variable(torch.from_numpy(X_test)).float()\n",
    "test_set_y = Variable(torch.LongTensor(y_test)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "epochs = 50\n",
    "for epochs in range(epochs):\n",
    "    model_opt.zero_grad()\n",
    "    out = model(train_set_X)\n",
    "    loss = loss_function(out, train_set_y)\n",
    "    loss.backward()\n",
    "    model_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/Vpy35/lib/python3.5/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "ans = model.predict(test_set_X)\n",
    "# for i in range(3299):\n",
    "#     ans_max,ind = torch.max(ans[i],0)\n",
    "#     print (test_set_y[i].numpy() - ind.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  4  1  7  2]\n",
      " [ 2  1  4  3  9]\n",
      " [ 3  1  4  2 10]\n",
      " ...\n",
      " [ 1  2  4  9  3]\n",
      " [ 2  1  4  3  9]\n",
      " [ 3  4  1  2 10]]\n",
      "[3. 2. 3. ... 9. 4. 3.]\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "y_pr = ans.numpy() \n",
    "y_pred = np.argsort(-y_pr,axis=1)[:,:k]\n",
    "print (y_pred)\n",
    "print (y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec@1: 0.454 prec@3: 0.669 prec@5: 0.765\n",
      "mprec@1: 1.509e-02 mprec@3: 6.147e-02 mprec@5: 1.019e-01\n",
      "prec matrix [0.45363636 0.59242424 0.66939394 0.71393939 0.76454545]\n",
      "mprec matrix [0.01508809 0.04888622 0.06147312 0.08406356 0.1019489 ]\n"
     ]
    }
   ],
   "source": [
    "# eval functions for Deep Learning\n",
    "preck = utils.getPrecAtK( y_test, y_pred, k )\n",
    "# The macro precision code takes a bit longer to execute due to the for loop over labels\n",
    "mpreck = utils.getMPrecAtK( y_test, y_pred, k )\n",
    "\n",
    "# According to our definitions, both prec@k and mprec@k should go up as k goes up i.e. for your\n",
    "# method, prec@i > prec@j if i > j and mprec@i > mprec@j if i > j. See the assignment description\n",
    "# to convince yourself why this must be the case.\n",
    "\n",
    "print( \"prec@1: %0.3f\" % preck[0], \"prec@3: %0.3f\" % preck[2], \"prec@5: %0.3f\" % preck[4] )\n",
    "# Dont be surprised if mprec is small -- it is hard to do well on rare error classes\n",
    "print( \"mprec@1: %0.3e\" % mpreck[0], \"mprec@3: %0.3e\" % mpreck[2], \"mprec@5: %0.3e\" % mpreck[4] )\n",
    "print (\"prec matrix\",preck)\n",
    "print (\"mprec matrix\",mpreck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 \n",
    "# same model diff train file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_latent_X = data_utils.TensorDataset(train_set_X, train_set_y)\n",
    "te_latent_X = data_utils.TensorDataset(test_set_X,  test_set_y)\n",
    "train_loader_X = torch.utils.data.DataLoader(dataset=tr_latent_X)\n",
    "test_loader_X = torch.utils.data.DataLoader(dataset=te_latent_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data,label) in enumerate(train_loader_X):\n",
    "\n",
    "        model_opt.zero_grad()\n",
    "\n",
    "        out = model(data)\n",
    "        loss = loss_function(out, label)\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        model_opt.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader_X.dataset),\n",
    "                100. * batch_idx / len(train_loader_X), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader_X.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6700 (0%)]\tLoss: 2.049522\n",
      "Train Epoch: 1 [100/6700 (1%)]\tLoss: 0.885262\n",
      "Train Epoch: 1 [200/6700 (3%)]\tLoss: 0.256305\n",
      "Train Epoch: 1 [300/6700 (4%)]\tLoss: 5.151550\n",
      "Train Epoch: 1 [400/6700 (6%)]\tLoss: 5.028206\n",
      "Train Epoch: 1 [500/6700 (7%)]\tLoss: 0.602708\n",
      "Train Epoch: 1 [600/6700 (9%)]\tLoss: 1.652547\n",
      "Train Epoch: 1 [700/6700 (10%)]\tLoss: 0.907288\n",
      "Train Epoch: 1 [800/6700 (12%)]\tLoss: 1.879640\n",
      "Train Epoch: 1 [900/6700 (13%)]\tLoss: 0.296418\n",
      "Train Epoch: 1 [1000/6700 (15%)]\tLoss: 3.316686\n",
      "Train Epoch: 1 [1100/6700 (16%)]\tLoss: 0.000075\n",
      "Train Epoch: 1 [1200/6700 (18%)]\tLoss: 20.039415\n",
      "Train Epoch: 1 [1300/6700 (19%)]\tLoss: 0.105142\n",
      "Train Epoch: 1 [1400/6700 (21%)]\tLoss: 0.421650\n",
      "Train Epoch: 1 [1500/6700 (22%)]\tLoss: 0.378309\n",
      "Train Epoch: 1 [1600/6700 (24%)]\tLoss: 4.647684\n",
      "Train Epoch: 1 [1700/6700 (25%)]\tLoss: 0.019948\n",
      "Train Epoch: 1 [1800/6700 (27%)]\tLoss: 1.970050\n",
      "Train Epoch: 1 [1900/6700 (28%)]\tLoss: 1.825400\n",
      "Train Epoch: 1 [2000/6700 (30%)]\tLoss: 4.814600\n",
      "Train Epoch: 1 [2100/6700 (31%)]\tLoss: 0.525765\n",
      "Train Epoch: 1 [2200/6700 (33%)]\tLoss: 1.528555\n",
      "Train Epoch: 1 [2300/6700 (34%)]\tLoss: 0.043081\n",
      "Train Epoch: 1 [2400/6700 (36%)]\tLoss: 0.838617\n",
      "Train Epoch: 1 [2500/6700 (37%)]\tLoss: 0.032101\n",
      "Train Epoch: 1 [2600/6700 (39%)]\tLoss: 0.841208\n",
      "Train Epoch: 1 [2700/6700 (40%)]\tLoss: 4.498678\n",
      "Train Epoch: 1 [2800/6700 (42%)]\tLoss: 1.366824\n",
      "Train Epoch: 1 [2900/6700 (43%)]\tLoss: 0.298355\n",
      "Train Epoch: 1 [3000/6700 (45%)]\tLoss: 1.486375\n",
      "Train Epoch: 1 [3100/6700 (46%)]\tLoss: 1.107109\n",
      "Train Epoch: 1 [3200/6700 (48%)]\tLoss: 0.165380\n",
      "Train Epoch: 1 [3300/6700 (49%)]\tLoss: 3.848707\n",
      "Train Epoch: 1 [3400/6700 (51%)]\tLoss: 0.080665\n",
      "Train Epoch: 1 [3500/6700 (52%)]\tLoss: 0.319840\n",
      "Train Epoch: 1 [3600/6700 (54%)]\tLoss: 1.657158\n",
      "Train Epoch: 1 [3700/6700 (55%)]\tLoss: 0.333291\n",
      "Train Epoch: 1 [3800/6700 (57%)]\tLoss: 0.198892\n",
      "Train Epoch: 1 [3900/6700 (58%)]\tLoss: 0.147633\n",
      "Train Epoch: 1 [4000/6700 (60%)]\tLoss: 5.790795\n",
      "Train Epoch: 1 [4100/6700 (61%)]\tLoss: 0.208956\n",
      "Train Epoch: 1 [4200/6700 (63%)]\tLoss: 0.036578\n",
      "Train Epoch: 1 [4300/6700 (64%)]\tLoss: 1.167042\n",
      "Train Epoch: 1 [4400/6700 (66%)]\tLoss: 1.608238\n",
      "Train Epoch: 1 [4500/6700 (67%)]\tLoss: 1.990529\n",
      "Train Epoch: 1 [4600/6700 (69%)]\tLoss: 2.571256\n",
      "Train Epoch: 1 [4700/6700 (70%)]\tLoss: 0.542740\n",
      "Train Epoch: 1 [4800/6700 (72%)]\tLoss: 2.085116\n",
      "Train Epoch: 1 [4900/6700 (73%)]\tLoss: 2.057611\n",
      "Train Epoch: 1 [5000/6700 (75%)]\tLoss: 3.015985\n",
      "Train Epoch: 1 [5100/6700 (76%)]\tLoss: 0.658445\n",
      "Train Epoch: 1 [5200/6700 (78%)]\tLoss: 2.067592\n",
      "Train Epoch: 1 [5300/6700 (79%)]\tLoss: 0.752497\n",
      "Train Epoch: 1 [5400/6700 (81%)]\tLoss: 0.155145\n",
      "Train Epoch: 1 [5500/6700 (82%)]\tLoss: 4.811197\n",
      "Train Epoch: 1 [5600/6700 (84%)]\tLoss: 5.350845\n",
      "Train Epoch: 1 [5700/6700 (85%)]\tLoss: 2.696417\n",
      "Train Epoch: 1 [5800/6700 (87%)]\tLoss: 2.008203\n",
      "Train Epoch: 1 [5900/6700 (88%)]\tLoss: 0.536445\n",
      "Train Epoch: 1 [6000/6700 (90%)]\tLoss: 0.910975\n",
      "Train Epoch: 1 [6100/6700 (91%)]\tLoss: 0.018943\n",
      "Train Epoch: 1 [6200/6700 (93%)]\tLoss: 0.413521\n",
      "Train Epoch: 1 [6300/6700 (94%)]\tLoss: 0.083356\n",
      "Train Epoch: 1 [6400/6700 (96%)]\tLoss: 1.932796\n",
      "Train Epoch: 1 [6500/6700 (97%)]\tLoss: 0.230353\n",
      "Train Epoch: 1 [6600/6700 (99%)]\tLoss: 0.168437\n",
      "====> Epoch: 1 Average loss: 1.5913\n",
      "Train Epoch: 2 [0/6700 (0%)]\tLoss: 0.315510\n",
      "Train Epoch: 2 [100/6700 (1%)]\tLoss: 0.094846\n",
      "Train Epoch: 2 [200/6700 (3%)]\tLoss: 0.110450\n",
      "Train Epoch: 2 [300/6700 (4%)]\tLoss: 0.415527\n",
      "Train Epoch: 2 [400/6700 (6%)]\tLoss: 3.865882\n",
      "Train Epoch: 2 [500/6700 (7%)]\tLoss: 0.112751\n",
      "Train Epoch: 2 [600/6700 (9%)]\tLoss: 0.683069\n",
      "Train Epoch: 2 [700/6700 (10%)]\tLoss: 0.386674\n",
      "Train Epoch: 2 [800/6700 (12%)]\tLoss: 1.491774\n",
      "Train Epoch: 2 [900/6700 (13%)]\tLoss: 0.084391\n",
      "Train Epoch: 2 [1000/6700 (15%)]\tLoss: 1.793030\n",
      "Train Epoch: 2 [1100/6700 (16%)]\tLoss: 3.218888\n",
      "Train Epoch: 2 [1200/6700 (18%)]\tLoss: 1.759394\n",
      "Train Epoch: 2 [1300/6700 (19%)]\tLoss: 0.106386\n",
      "Train Epoch: 2 [1400/6700 (21%)]\tLoss: 0.031316\n",
      "Train Epoch: 2 [1500/6700 (22%)]\tLoss: 0.182456\n",
      "Train Epoch: 2 [1600/6700 (24%)]\tLoss: 0.002997\n",
      "Train Epoch: 2 [1700/6700 (25%)]\tLoss: 0.005113\n",
      "Train Epoch: 2 [1800/6700 (27%)]\tLoss: 0.595536\n",
      "Train Epoch: 2 [1900/6700 (28%)]\tLoss: 1.326064\n",
      "Train Epoch: 2 [2000/6700 (30%)]\tLoss: 6.092058\n",
      "Train Epoch: 2 [2100/6700 (31%)]\tLoss: 0.132278\n",
      "Train Epoch: 2 [2200/6700 (33%)]\tLoss: 1.114376\n",
      "Train Epoch: 2 [2300/6700 (34%)]\tLoss: 0.032910\n",
      "Train Epoch: 2 [2400/6700 (36%)]\tLoss: 0.133307\n",
      "Train Epoch: 2 [2500/6700 (37%)]\tLoss: 0.028705\n",
      "Train Epoch: 2 [2600/6700 (39%)]\tLoss: 0.746307\n",
      "Train Epoch: 2 [2700/6700 (40%)]\tLoss: 2.541287\n",
      "Train Epoch: 2 [2800/6700 (42%)]\tLoss: 0.270704\n",
      "Train Epoch: 2 [2900/6700 (43%)]\tLoss: 0.024365\n",
      "Train Epoch: 2 [3000/6700 (45%)]\tLoss: 1.104302\n",
      "Train Epoch: 2 [3100/6700 (46%)]\tLoss: 0.454928\n",
      "Train Epoch: 2 [3200/6700 (48%)]\tLoss: 0.102457\n",
      "Train Epoch: 2 [3300/6700 (49%)]\tLoss: 2.995719\n",
      "Train Epoch: 2 [3400/6700 (51%)]\tLoss: 0.048140\n",
      "Train Epoch: 2 [3500/6700 (52%)]\tLoss: 2.788409\n",
      "Train Epoch: 2 [3600/6700 (54%)]\tLoss: 2.746816\n",
      "Train Epoch: 2 [3700/6700 (55%)]\tLoss: 0.116635\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3 12 22  4  7]\n",
      " [ 2  4  1  9 21]\n",
      " [ 3 12  4  1 39]\n",
      " ...\n",
      " [ 9  2 45 15 16]\n",
      " [ 4 37  5 18  2]\n",
      " [ 3 12 37 22 10]]\n",
      "[3. 2. 3. ... 9. 4. 3.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/Vpy35/lib/python3.5/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "ans = model.predict(test_set_X)\n",
    "k = 5\n",
    "y_pr = ans.numpy() \n",
    "y_pred = np.argsort(-y_pr,axis=1)[:,:k]\n",
    "print (y_pred)\n",
    "print (y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec@1: 0.762 prec@3: 0.918 prec@5: 0.955\n",
      "mprec@1: 8.627e-03 mprec@3: 6.489e-02 mprec@5: 1.113e-01\n",
      "prec matrix [0.76212121 0.88       0.91757576 0.94030303 0.95545455]\n",
      "mprec matrix [0.00862691 0.0407148  0.06489299 0.09325235 0.11130134]\n"
     ]
    }
   ],
   "source": [
    "# eval functions for Deep Learning\n",
    "preck = utils.getPrecAtK( y_test, y_pred, k )\n",
    "# The macro precision code takes a bit longer to execute due to the for loop over labels\n",
    "mpreck = utils.getMPrecAtK( y_test, y_pred, k )\n",
    "\n",
    "# According to our definitions, both prec@k and mprec@k should go up as k goes up i.e. for your\n",
    "# method, prec@i > prec@j if i > j and mprec@i > mprec@j if i > j. See the assignment description\n",
    "# to convince yourself why this must be the case.\n",
    "\n",
    "print( \"prec@1: %0.3f\" % preck[0], \"prec@3: %0.3f\" % preck[2], \"prec@5: %0.3f\" % preck[4] )\n",
    "# Dont be surprised if mprec is small -- it is hard to do well on rare error classes\n",
    "print( \"mprec@1: %0.3e\" % mpreck[0], \"mprec@3: %0.3e\" % mpreck[2], \"mprec@5: %0.3e\" % mpreck[4] )\n",
    "print (\"prec matrix\",preck)\n",
    "print (\"mprec matrix\",mpreck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
