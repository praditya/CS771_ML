{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import predict\n",
    "import time as tm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.datasets import make_classification\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/Vpy35/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# from w2v import *\n",
    "# from embedding_layer import embedding_layer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.io as sio\n",
    "from scipy import sparse\n",
    "import argparse\n",
    "from visdom import Visdom\n",
    "from sklearn.externals import joblib \n",
    "# from futils import *\n",
    "# from loss import loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Data\n",
    "dictSize = 225\n",
    "(X, y) = utils.loadData( \"train\", dictSize = dictSize )\n",
    "X = scipy.sparse.csr_matrix.toarray(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML_CNN github xmlCNN.py\n",
    "class xmlCNN(nn.Module):\n",
    "    def __init__(self, params, embedding_weights):\n",
    "        super(xmlCNN, self).__init__()\n",
    "        self.params = params\n",
    "        self.embedding_layer = embedding_layer(params, embedding_weights)\n",
    "        self.classifier = cnn_encoder(params)\n",
    "        \n",
    "    def forward(self, batch_x, batch_y):\n",
    "        # ----------- Encode (X, Y) --------------------------------------------\n",
    "        e_emb = self.embedding_layer.forward(batch_x)\n",
    "        Y = self.classifier.forward(e_emb)\n",
    "        loss = self.params.loss_fn(Y, batch_y)\n",
    "        \n",
    "        if(loss<0):\n",
    "            print(cross_entropy)\n",
    "            print(Y[0:100])\n",
    "            print(batch_y[0:100])\n",
    "            sys.exit()\n",
    "\n",
    "        return loss.view(-1,1), Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML_CNN github embedding_layer.py\n",
    "class embedding_layer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, params, embedding_weights):\n",
    "        super(embedding_layer, self).__init__()\n",
    "        self.l = nn.Embedding(params.vocab_size, params.embedding_dim)\n",
    "        if params.model_variation == 'pretrain':\n",
    "            self.l.weight.data.copy_(torch.from_numpy(embedding_weights))\n",
    "            self.l.weight.requires_grad=False\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        o = self.l(inputs)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML_CNN github cnn_encoder.py\n",
    "def out_size(l_in, kernel_size, padding=0, dilation=1, stride=1):\n",
    "    a = l_in + 2*padding - dilation*(kernel_size - 1) - 1\n",
    "    b = int(a/stride)\n",
    "    return b + 1\n",
    "\n",
    "class cnn_encoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super(cnn_encoder, self).__init__()\n",
    "        self.params = params\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.pool_layers = nn.ModuleList()\n",
    "        fin_l_out_size = 0\n",
    "        \n",
    "        if(params.dropouts):\n",
    "            self.drp = nn.Dropout(p=.25)\n",
    "            self.drp5 = nn.Dropout(p=.5)\n",
    "\n",
    "        for fsz in params.filter_sizes:\n",
    "            l_out_size = out_size(params.sequence_length, fsz, stride=2)\n",
    "            pool_size = l_out_size // params.pooling_units\n",
    "            l_conv = nn.Conv1d(params.embedding_dim, params.num_filters, fsz, stride=2)\n",
    "            torch.nn.init.xavier_uniform_(l_conv.weight)\n",
    "            if params.pooling_type == 'average':\n",
    "                l_pool = nn.AvgPool1d(pool_size, stride=None, count_include_pad=True)\n",
    "                pool_out_size = (int((l_out_size - pool_size)/pool_size) + 1)*params.num_filters\n",
    "            elif params.pooling_type == 'max':\n",
    "                l_pool = nn.MaxPool1d(2, stride=1)\n",
    "                pool_out_size = (int(l_out_size*params.num_filters - 2) + 1)\n",
    "            fin_l_out_size += pool_out_size\n",
    "\n",
    "            self.conv_layers.append(l_conv)\n",
    "            self.pool_layers.append(l_pool)\n",
    "\n",
    "        self.fin_layer = nn.Linear(fin_l_out_size, params.hidden_dims)\n",
    "        self.out_layer = nn.Linear(params.hidden_dims, params.y_dim)\n",
    "        torch.nn.init.xavier_uniform_(self.fin_layer.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.out_layer.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #o0 = self.drp(self.bn_1(inputs)).permute(0,2,1)\n",
    "        o0 = inputs.permute(0,2,1)# self.bn_1(inputs.permute(0,2,1))\n",
    "        if(self.params.dropouts):\n",
    "            o0 = self.drp(o0) \n",
    "        conv_out = []\n",
    "\n",
    "        for i in range(len(self.params.filter_sizes)):\n",
    "            o = self.conv_layers[i](o0)\n",
    "            o = o.view(o.shape[0], 1, o.shape[1]*o.shape[2])\n",
    "            o = self.pool_layers[i](o)\n",
    "            o = nn.functional.relu(o)\n",
    "            o = o.view(o.shape[0],-1)\n",
    "            conv_out.append(o)\n",
    "            del o\n",
    "        if len(self.params.filter_sizes)>1:\n",
    "            o = torch.cat(conv_out,1)\n",
    "        else:\n",
    "            o = conv_out[0]\n",
    "\n",
    "        o = self.fin_layer(o)\n",
    "        o = nn.functional.relu(o)\n",
    "        if(self.params.dropouts):\n",
    "            o = self.drp5(o) \n",
    "        o = self.out_layer(o)\n",
    "        o = torch.nn.functional.sigmoid(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML_CNN github classifier.py\n",
    "class classifier(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(classifier, self).__init__()\n",
    "        self.params = params\n",
    "        if(self.params.dropouts):\n",
    "            self.drp = nn.Dropout(.5)\n",
    "        self.l1 = nn.Linear(params.h_dim, params.H_dim)\n",
    "        self.l2 = nn.Linear(params.H_dim, params.y_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        torch.nn.init.xavier_uniform_(self.l1.weight)\n",
    "\n",
    "    def forward(self, H):\n",
    "        H = self.l1(H)\n",
    "        H = self.relu(H)\n",
    "        H = self.l2(H)\n",
    "        H = self.sigmoid(H)\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "# ------------------------ Params -------------------------------------------------------------------------------\n",
    "parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "\n",
    "parser.add_argument('--zd', dest='Z_dim', type=int, default=100, help='Latent layer dimension')\n",
    "parser.add_argument('--mb', dest='mb_size', type=int, default=20, help='Size of minibatch, changing might result in latent layer variance overflow')\n",
    "# parser.add_argument('--hd', dest='h_dim', type=int, default=600, help='hidden layer dimension')\n",
    "parser.add_argument('--lr', dest='lr', type=int, default=1e-3, help='Learning Rate')\n",
    "parser.add_argument('--p', dest='plot_flg', type=int, default=0, help='1 to plot, 0 to not plot')\n",
    "parser.add_argument('--e', dest='num_epochs', type=int, default=100, help='step for displaying loss')\n",
    "\n",
    "parser.add_argument('--d', dest='disp_flg', type=int, default=0, help='display graphs')\n",
    "parser.add_argument('--sve', dest='save', type=int, default=1, help='save models or not')\n",
    "parser.add_argument('--ss', dest='save_step', type=int, default=10, help='gap between model saves')\n",
    "parser.add_argument('--mn', dest='model_name', type=str, default='', help='model name')\n",
    "parser.add_argument('--tr', dest='training', type=int, default=1, help='model name')\n",
    "parser.add_argument('--lm', dest='load_model', type=str, default=\"\", help='model name')\n",
    "parser.add_argument('--ds', dest='data_set', type=str, default=\"rcv\", help='dataset name')\n",
    "\n",
    "parser.add_argument('--pp', dest='pp_flg', type=int, default=0, help='1 is for min-max pp, 2 is for gaussian pp, 0 for none')\n",
    "parser.add_argument('--loss', dest='loss_type', type=str, default=\"BCELoss\", help='Loss')\n",
    "\n",
    "parser.add_argument('--hidden_dims', type=int, default=512, help='hidden layer dimension')\n",
    "parser.add_argument('--sequence_length',help='max sequence length of a document', type=int,default=500)\n",
    "parser.add_argument('--embedding_dim', help='dimension of word embedding representation', type=int, default=300)\n",
    "parser.add_argument('--model_variation', help='model variation: CNN-rand or CNN-pretrain', type=str, default='CNN-rand')\n",
    "parser.add_argument('--pretrain_type', help='pretrain model: GoogleNews or glove', type=str, default='glove')\n",
    "parser.add_argument('--vocab_size', help='size of vocabulary keeping the most frequent words', type=int, default=30000)\n",
    "parser.add_argument('--drop_prob', help='Dropout probability', type=int, default=.3)\n",
    "parser.add_argument('--load_data', help='Load Data or not', type=int, default=0)\n",
    "parser.add_argument('--mg', dest='multi_gpu', type=int, default=0, help='1 for 2 gpus and 0 for normal')\n",
    "parser.add_argument('--filter_sizes', help='number of filter sizes (could be a list of integer)', type=int, default=[2, 4, 8], nargs='+')\n",
    "parser.add_argument('--num_filters', help='number of filters (i.e. kernels) in CNN model', type=int, default=32)\n",
    "parser.add_argument('--pooling_units', help='number of pooling units in 1D pooling layer', type=int, default=32)\n",
    "parser.add_argument('--pooling_type', help='max or average', type=str, default='max')\n",
    "parser.add_argument('--model_type', help='glove or GoogleNews', type=str, default='glove')\n",
    "parser.add_argument('--num_features', help='50, 100, 200, 300', type=int, default=300)\n",
    "parser.add_argument('--dropouts', help='0 for not using, 1 for using', type=int, default=0)\n",
    "parser.add_argument('--clip', help='gradient clipping', type=float, default=1000)\n",
    "parser.add_argument('--dataset_gpu', help='load dataset in full to gpu', type=int, default=1)\n",
    "parser.add_argument('--dp', dest='dataparallel', help='to train on multiple GPUs or not', type=int, default=0)\n",
    "\n",
    "# dummy argument to avoid error since using Ipython and argparse already been parsed\n",
    "parser.add_argument('-f') \n",
    "\n",
    "params = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(Z_dim=100, clip=1000, data_set='rcv', dataparallel=0, dataset_gpu=1, disp_flg=0, drop_prob=0.3, dropouts=0, embedding_dim=300, f='/home/aditya/.local/share/jupyter/runtime/kernel-5b081fa4-859d-4d1b-b457-5d824cb6cc47.json', filter_sizes=[2, 4, 8], hidden_dims=512, load_data=0, load_model='', loss_type='BCELoss', lr=0.001, mb_size=20, model_name='', model_type='glove', model_variation='CNN-rand', multi_gpu=0, num_epochs=100, num_features=300, num_filters=32, plot_flg=0, pooling_type='max', pooling_units=32, pp_flg=0, pretrain_type='glove', save=1, save_step=10, sequence_length=500, training=1, vocab_size=30000)\n"
     ]
    }
   ],
   "source": [
    "# define Params\n",
    "print (params)\n",
    "params.loss_fn = torch.nn.BCELoss(size_average=False)\n",
    "embedding_weights = None\n",
    "if torch.cuda.is_available():\n",
    "    params.dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    params.dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params from saveload data\n",
    "# y_train.shape[1]\n",
    "params.X_dim = X_train.shape[1]\n",
    "params.y_dim = 1\n",
    "params.N = X_train.shape[0]\n",
    "params.vocab_size = 50\n",
    "params.classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model to: Gen_data_CNN_Z_dim-100_mb_size-20_hidden_dims-512_preproc-0_loss-BCELoss_sequence_length-500_embedding_dim-300_params.vocab_size=50\n"
     ]
    }
   ],
   "source": [
    "if(len(params.model_name)==0):\n",
    "    params.model_name = \"Gen_data_CNN_Z_dim-{}_mb_size-{}_hidden_dims-{}_preproc-{}_loss-{}_sequence_length-{}_embedding_dim-{}_params.vocab_size={}\".format(params.Z_dim, params.mb_size, params.hidden_dims, params.pp_flg, params.loss_type, params.sequence_length, params.embedding_dim, params.vocab_size)\n",
    "\n",
    "print('Saving Model to: ' + params.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# def pass(a, b, model, x_tr, Y, params):\n",
    "#     # e_emb = model.embedding_layer.forward(x_tr[i:i+params.mb_size].view(params.mb_size, x_te.shape[1]))\n",
    "#     # Y[i:i+params.mb_size,:] = model.classifier(e_emb).data\n",
    "#     e_emb = model.embedding_layer.forward(x_tr[a:b].view(params.mb_size, x_tr.shape[1]))\n",
    "#     Y[a:b,:] = model.classifier(e_emb).data\n",
    "\n",
    "#     return Y\n",
    "\n",
    "def test_class(x_te, y_te, params, model=None, x_tr=None, y_tr=None, embedding_weights=None, verbose=True, save=True ):\n",
    "\n",
    "    \n",
    "    if(model==None):\n",
    "        if(embedding_weights is None):\n",
    "            print(\"Error: Embedding weights needed!\")\n",
    "            exit()\n",
    "        else:\n",
    "            model = xmlCNN(params, embedding_weights)\n",
    "            # state_dict = torch.load(params.load_model + \"/model_best\", map_location=lambda storage, loc: storage)\n",
    "            # new_state_dict = OrderedDict()\n",
    "            # for k, v in state_dict.items():\n",
    "            #     name = k[7:]\n",
    "            #     new_state_dict[name] = v\n",
    "            # model.load_state_dict(new_state_dict)\n",
    "            # del new_state_dict\n",
    "            model = load_model(model, params.load_model)\n",
    "            \n",
    "    if(torch.cuda.is_available()):\n",
    "        params.dtype_f = torch.cuda.FloatTensor\n",
    "        params.dtype_i = torch.cuda.LongTensor\n",
    "        model = model.cuda()\n",
    "    else:\n",
    "        params.dtype_f = torch.FloatTensor\n",
    "        params.dtype_i = torch.LongTensor\n",
    "\n",
    "    if(x_tr is not None and y_tr is not None):\n",
    "        x_tr, _ = load_batch_cnn(x_tr, y_tr, params, batch=False)\n",
    "        Y = np.zeros(y_tr.shape)\n",
    "        rem = x_tr.shape[0]%params.mb_size \n",
    "        for i in range(0, x_tr.shape[0] - rem, params.mb_size ):\n",
    "            e_emb = model.embedding_layer.forward(x_tr[i:i+params.mb_size].view(params.mb_size, x_te.shape[1]))\n",
    "            Y[i:i+params.mb_size,:] = model.classifier(e_emb).data\n",
    "    if(rem):\n",
    "        e_emb = model.embedding_layer.forward(x_tr[-rem:].view(rem, x_te.shape[1]))\n",
    "        Y[-rem:, :] = model.classifier(e_emb).data\n",
    "        \n",
    "    loss = log_loss(y_tr, Y)\n",
    "    prec = precision_k(y_tr.todense(), Y, 5)\n",
    "    print('Test Loss; Precision Scores [1->5] {} {} {} {} {} Cross Entropy {};'.format(prec[0], prec[1], prec[2], prec[3], prec[4],loss))\n",
    "    \n",
    "    \n",
    "    x_te, _ = load_batch_cnn(x_te, y_te, params, batch=False)\n",
    "    Y2 = np.zeros(y_te.shape)\n",
    "    rem = x_te.shape[0]%params.mb_size\n",
    "    for i in range(0,x_te.shape[0] - rem,params.mb_size):\n",
    "        e_emb = model.embedding_layer.forward(x_te[i:i+params.mb_size].view(params.mb_size, x_te.shape[1]))\n",
    "        Y2[i:i+params.mb_size,:] = model.classifier(e_emb).data\n",
    "\n",
    "    if(rem):\n",
    "        e_emb = model.embedding_layer.forward(x_te[-rem:].view(rem, x_te.shape[1]))\n",
    "        Y2[-rem:,:] = model.classifier(e_emb).data\n",
    "\n",
    "    loss = log_loss(y_te, Y2) # Reverse of pytorch\n",
    "    #print(\"A\")\n",
    "    prec = precision_k(y_te.todense(), Y2, 5) # Reverse of pytorch\n",
    "    print('Test Loss; Precision Scores [1->5] {} {} {} {} {} Cross Entropy {};'.format(prec[0], prec[1], prec[2], prec[3], prec[4],loss))\n",
    "    \n",
    "    if(save):\n",
    "        Y_probabs2 = sparse.csr_matrix(Y2)\n",
    "        sio.savemat('/'.join(params.load_model.split('/')[-1]) + '/score_matrix.mat' , {'score_matrix': Y_probabs2})\n",
    "\n",
    "    return prec[0], loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_cnn(x_tr, y_tr, params, batch=True, batch_size=0, decoder_word_input=None, decoder_target=None, testing=0):\n",
    "\n",
    "    indexes = 0 # for scope\n",
    "    if(batch):\n",
    "        if(batch_size):\n",
    "            params.go_row = np.ones((batch_size,1))*params.vocabulary[params.go_token]\n",
    "            params.end_row = np.ones((batch_size,1))*params.vocabulary[params.end_token]\n",
    "            indexes = np.array(np.random.randint(x_tr.shape[0], size=batch_size))\n",
    "            x_tr, y_tr = x_tr[indexes,:], y_tr[indexes,:]\n",
    "        else:\n",
    "            params.go_row = np.ones((params.mb_size,1))*params.vocabulary[params.go_token]\n",
    "            params.end_row = np.ones((params.mb_size,1))*params.vocabulary[params.end_token]\n",
    "            indexes = np.array(np.random.randint(x_tr.shape[0], size=params.mb_size))\n",
    "            x_tr, y_tr = x_tr[indexes,:], y_tr[indexes,:]\n",
    "    else:\n",
    "        params.go_row = np.ones((x_tr.shape[0],1))*params.vocabulary[params.go_token]\n",
    "        params.end_row = np.ones((x_tr.shape[0],1))*params.vocabulary[params.end_token]\n",
    "\n",
    "    x_tr = x_tr.todense()\n",
    "    y_tr = y_tr.todense()\n",
    "\n",
    "    x_tr = Variable(torch.from_numpy(x_tr.astype('int')).type(params.dtype_i))\n",
    "    if(testing==0):\n",
    "        y_tr = Variable(torch.from_numpy(y_tr.astype('float')).type(params.dtype_f))\n",
    "\n",
    "    return x_tr, y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_tr, y_tr, x_te, y_te, embedding_weights, params):\n",
    "\t\t\n",
    "\tviz = Visdom()\n",
    "\tloss_best = float('Inf')\n",
    "\tbestTotalLoss = float('Inf')\n",
    "\tbest_test_acc = 0\n",
    "\tmax_grad = 0\n",
    "\n",
    "\tnum_mb = np.ceil(params.N/params.mb_size)\n",
    "\t\n",
    "\tmodel = xmlCNN(params, embedding_weights)\n",
    "\tif(torch.cuda.is_available()):\n",
    "\t\tprint(\"--------------- Using GPU! ---------\")\n",
    "\t\tmodel.params.dtype_f = torch.cuda.FloatTensor\n",
    "\t\tmodel.params.dtype_i = torch.cuda.LongTensor\n",
    "\t\t\n",
    "\t\tmodel = model.cuda()\n",
    "\telse:\n",
    "\t\tmodel.params.dtype_f = torch.FloatTensor\n",
    "\t\tmodel.params.dtype_i = torch.LongTensor\n",
    "\t\tprint(\"=============== Using CPU =========\")\n",
    "\n",
    "\toptimizer = optim.Adam(filter(lambda p: p.requires_grad,model.parameters()), lr=params.lr)\n",
    "\tprint(model);print(\"%\"*100)\n",
    "\t\n",
    "\tif params.dataparallel:\n",
    "\t\tmodel = nn.DataParallel(model)\n",
    "\t\n",
    "\tif(len(params.load_model)):\n",
    "\t\tparams.model_name = params.load_model\n",
    "\t\tprint(params.load_model)\n",
    "\t\tmodel, optimizer, init = load_model(model, params.load_model, optimizer=optimizer)\n",
    "\telse:\n",
    "\t\tinit = 0\n",
    "\titeration = 0\n",
    "\t# =============================== TRAINING ====================================\n",
    "\tfor epoch in range(init, params.num_epochs):\n",
    "\t\ttotalLoss = 0.0\n",
    "\n",
    "\t\tfor i in range(int(num_mb)):\n",
    "\t\t\t# ------------------ Load Batch Data ---------------------------------------------------------\n",
    "\t\t\tbatch_x, batch_y = load_batch_cnn(x_tr, y_tr, params)\n",
    "\t\t\t# -----------------------------------------------------------------------------------\n",
    "\t\t\tloss, output = model.forward(batch_x, batch_y)\n",
    "\t\t\tloss = loss.mean().squeeze()\n",
    "\t\t\t# --------------------------------------------------------------------\n",
    "\n",
    "\t\t\ttotalLoss += loss.data\n",
    "\t\t\t\n",
    "\t\t\tif i % int(num_mb/12) == 0:\n",
    "\t\t\t\tprint('Iter-{}; Loss: {:.4}; best_loss: {:.4}; max_grad: {}:'.format(i, loss.data, loss_best, max_grad))\n",
    "\t\t\t\tif not os.path.exists('../saved_models/' + params.model_name ):\n",
    "\t\t\t\t\tos.makedirs('../saved_models/' + params.model_name)\n",
    "\t\t\t\tsave_model(model, optimizer, epoch, params.model_name + \"/model_best_batch\")\n",
    "\t\t\t\tif(loss<loss_best):\n",
    "\t\t\t\t\tloss_best = loss.data\n",
    "\n",
    "\t\t\t# ------------------------ Propogate loss -----------------------------------\n",
    "\t\t\tloss.backward()\n",
    "\t\t\tloss = loss.data\n",
    "\t\t\ttorch.nn.utils.clip_grad_norm(model.parameters(), params.clip)\n",
    "\t\t\tsm = 0\n",
    "\t\t\tsm2=0\n",
    "\t\t\tmax_grad = 0\n",
    "\t\t\tfor p in model.parameters():\n",
    "\t\t\t\tif(p.grad is not None):\n",
    "\t\t\t\t\tmax_grad = max(torch.max(p.grad).data[0], max_grad)\n",
    "\t\t\t\t\tsm += p.grad.view(-1).shape[0]\n",
    "\t\t\t\t\tsm2 = p.grad.mean().squeeze()*p.grad.view(-1).shape[0]\n",
    "\t\t\tavg_grad = (sm2/sm).data[0]\n",
    "\t\t\t# optimizer.step()\n",
    "\t\t\tif(torch.__version__ == '0.4.0'):\n",
    "\t\t\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), params.clip)\n",
    "\t\t\telse:\n",
    "\t\t\t\t\ttorch.nn.utils.clip_grad_norm(model.parameters(), params.clip)\n",
    "\t\t\tfor p in model.parameters():\n",
    "\t\t\t\t\tif(p.grad is not None):\n",
    "\t\t\t\t\t\t\tp.data.add_(-params.lr, p.grad.data)\n",
    "\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t# ----------------------------------------------------------------------------\n",
    "\t\t\tif(params.disp_flg):\n",
    "\t\t\t\tif(iteration==0):\n",
    "\t\t\t\t\tloss_old = loss\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tviz.line(X=np.linspace(iteration-1,iteration,50), Y=np.linspace(loss_old, loss,50), update='append', win=win)\n",
    "\t\t\t\t\tloss_old = loss\n",
    "\t\t\t\tif(iteration % 100 == 0 ):\n",
    "\t\t\t\t\twin = viz.line(X=np.arange(iteration, iteration + .1), Y=np.arange(0, .1))\n",
    "\t\t\titeration +=1\n",
    "\n",
    "\t\t\tif(epoch==0):\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\tif(totalLoss<bestTotalLoss):\n",
    "\n",
    "\t\t\tbestTotalLoss = totalLoss\n",
    "\t\t\tif not os.path.exists('../saved_models/' + params.model_name ):\n",
    "\t\t\t\tos.makedirs('../saved_models/' + params.model_name)\n",
    "\t\t\tsave_model(model, optimizer, epoch, params.model_name + \"/model_best_epoch\")\n",
    "\n",
    "\t\tprint('End-of-Epoch: {} Loss: {:.4}; best_loss: {:.4};'.format(epoch, totalLoss, bestTotalLoss))\n",
    "\t\n",
    "\t\t# import pdb\n",
    "\t\t# pdb.set_trace()\n",
    "\t\ttest_prec_acc, test_ce_loss = test_class(x_te, y_te, params, model=model, verbose=False, save=False)\n",
    "\t\tmodel.train()\n",
    "\t\t\n",
    "\t\tif(test_prec_acc > best_test_acc):\n",
    "\t\t\tbest_test_loss = test_ce_loss\n",
    "\t\t\tbest_test_acc = test_prec_acc\n",
    "\t\t\tprint(\"This acc is better than the previous recored test acc:- {} ; while CELoss:- {}\".format(best_test_acc, best_test_loss))\n",
    "\t\t\tif not os.path.exists('../saved_models/' + params.model_name ):\n",
    "\t\t\t\tos.makedirs('../saved_models/' + params.model_name)\n",
    "\t\t\tsave_model(model, optimizer, epoch, params.model_name + \"/model_best_test\")\n",
    "\n",
    "\t\tif epoch % params.save_step == 0:\n",
    "\t\t\tsave_model(model, optimizer, epoch, params.model_name + \"/model_\" + str(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aditya/Vpy35/lib/python3.5/site-packages/urllib3/connection.py\", line 157, in _new_conn\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw\n",
      "  File \"/home/aditya/Vpy35/lib/python3.5/site-packages/urllib3/util/connection.py\", line 84, in create_connection\n",
      "    raise err\n",
      "  File \"/home/aditya/Vpy35/lib/python3.5/site-packages/urllib3/util/connection.py\", line 74, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aditya/Vpy35/lib/python3.5/site-packages/urllib3/connectionpool.py\", line 672, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"/home/aditya/Vpy35/lib/python3.5/site-packages/urllib3/connectionpool.py\", line 387, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/usr/lib/python3.5/http/client.py\", line 1122, in request\n",
      "    self._send_request(method, url, body, headers)\n",
      "  File \"/usr/lib/python3.5/http/client.py\", line 1167, in _send_request\n",
      "    self.endheaders(body)\n",
      "  File \"/usr/lib/python3.5/http/client.py\", line 1118, in endheaders\n",
      "    self._send_output(message_body)\n",
      "  File \"/usr/lib/python3.5/http/client.py\", line 944, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/usr/lib/python3.5/http/client.py\", line 887, in send\n",
      "    self.connect()\n",
      "  File \"/home/aditya/Vpy35/lib/python3.5/site-packages/urllib3/connection.py\", line 184, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/home/aditya/Vpy35/lib/python3.5/site-packages/urllib3/connection.py\", line 169, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f5804498d68>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aditya/Vpy35/lib/python3.5/site-packages/requests/adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"/home/aditya/Vpy35/lib/python3.5/site-packages/urllib3/connectionpool.py\", line 720, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"/home/aditya/Vpy35/lib/python3.5/site-packages/urllib3/util/retry.py\", line 436, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f5804498d68>: Failed to establish a new connection: [Errno 111] Connection refused',))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aditya/Vpy35/lib/python3.5/site-packages/visdom/__init__.py\", line 711, in _send\n",
      "    data=json.dumps(msg),\n",
      "  File \"/home/aditya/Vpy35/lib/python3.5/site-packages/visdom/__init__.py\", line 677, in _handle_post\n",
      "    r = self.session.post(url, data=data)\n",
      "  File \"/home/aditya/Vpy35/lib/python3.5/site-packages/requests/sessions.py\", line 578, in post\n",
      "    return self.request('POST', url, data=data, json=json, **kwargs)\n",
      "  File \"/home/aditya/Vpy35/lib/python3.5/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/aditya/Vpy35/lib/python3.5/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/aditya/Vpy35/lib/python3.5/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f5804498d68>: Failed to establish a new connection: [Errno 111] Connection refused',))\n",
      "[Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in user code:\n",
      "------------------------------------------------------------\n",
      "=============== Using CPU =========\n",
      "xmlCNN(\n",
      "  (embedding_layer): embedding_layer(\n",
      "    (l): Embedding(50, 300)\n",
      "  )\n",
      "  (classifier): cnn_encoder(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): Conv1d(300, 32, kernel_size=(2,), stride=(2,))\n",
      "      (1): Conv1d(300, 32, kernel_size=(4,), stride=(2,))\n",
      "      (2): Conv1d(300, 32, kernel_size=(8,), stride=(2,))\n",
      "    )\n",
      "    (pool_layers): ModuleList(\n",
      "      (0): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (fin_layer): Linear(in_features=23869, out_features=512, bias=True)\n",
      "    (out_layer): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'vocabulary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-175c0f7b8aa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-20c803443983>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x_tr, y_tr, x_te, y_te, embedding_weights, params)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                         \u001b[0;31m# ------------------ Load Batch Data ---------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_batch_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                         \u001b[0;31m# -----------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-6c728e435271>\u001b[0m in \u001b[0;36mload_batch_cnn\u001b[0;34m(x_tr, y_tr, params, batch, batch_size, decoder_word_input, decoder_target, testing)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgo_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgo_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'vocabulary'"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, X_test, y_test, embedding_weights, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
