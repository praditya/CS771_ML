{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import predict\n",
    "import time as tm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.datasets import make_classification\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Data\n",
    "dictSize = 225\n",
    "(X, y) = utils.loadData( \"train\", dictSize = dictSize )\n",
    "X = scipy.sparse.csr_matrix.toarray(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      float64\n",
       "1      float64\n",
       "2      float64\n",
       "3      float64\n",
       "4      float64\n",
       "5      float64\n",
       "6      float64\n",
       "7      float64\n",
       "8      float64\n",
       "9      float64\n",
       "10     float64\n",
       "11     float64\n",
       "12     float64\n",
       "13     float64\n",
       "14     float64\n",
       "15     float64\n",
       "16     float64\n",
       "17     float64\n",
       "18     float64\n",
       "19     float64\n",
       "20     float64\n",
       "21     float64\n",
       "22     float64\n",
       "23     float64\n",
       "24     float64\n",
       "25     float64\n",
       "26     float64\n",
       "27     float64\n",
       "28     float64\n",
       "29     float64\n",
       "        ...   \n",
       "195    float64\n",
       "196    float64\n",
       "197    float64\n",
       "198    float64\n",
       "199    float64\n",
       "200    float64\n",
       "201    float64\n",
       "202    float64\n",
       "203    float64\n",
       "204    float64\n",
       "205    float64\n",
       "206    float64\n",
       "207    float64\n",
       "208    float64\n",
       "209    float64\n",
       "210    float64\n",
       "211    float64\n",
       "212    float64\n",
       "213    float64\n",
       "214    float64\n",
       "215    float64\n",
       "216    float64\n",
       "217    float64\n",
       "218    float64\n",
       "219    float64\n",
       "220    float64\n",
       "221    float64\n",
       "222    float64\n",
       "223    float64\n",
       "224    float64\n",
       "Length: 225, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X)\n",
    "# print (df)\n",
    "df.shape\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class nn https://stackabuse.com/introduction-to-pytorch-for-classification/\n",
    "# Model for predictions\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, num_numerical_cols, output_size, layers, p=0.4):\n",
    "        super().__init__()\n",
    "        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
    "        self.embedding_dropout = nn.Dropout(p)\n",
    "        self.batch_norm_num = nn.BatchNorm1d(num_numerical_cols)\n",
    "\n",
    "        all_layers = []\n",
    "        num_categorical_cols = sum((nf for ni, nf in embedding_size))\n",
    "        input_size = num_categorical_cols + num_numerical_cols\n",
    "\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i\n",
    "\n",
    "        all_layers.append(nn.Linear(layers[-1], output_size))\n",
    "\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x_categorical, x_numerical):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.all_embeddings):\n",
    "            embeddings.append(e(x_categorical[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x_numerical = self.batch_norm_num(x_numerical)\n",
    "        x = torch.cat([x, x_numerical], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'categorical_embedding_sizes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b9d3a401e641>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorical_embedding_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumerical_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'categorical_embedding_sizes' is not defined"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "# create model\n",
    "model = Model(categorical_embedding_sizes, numerical_data.shape[1], 2, [200,100,50], p=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-92af3d436349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# loss function and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-3234c9029c25>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-3234c9029c25>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\u001b[0m\n\u001b[0m                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "epochs = 300\n",
    "aggregated_losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(categorical_train_data, numerical_train_data)\n",
    "    single_loss = loss_function(y_pred, train_outputs)\n",
    "    aggregated_losses.append(single_loss)\n",
    "\n",
    "    if i%25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-4776c476ee82>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-4776c476ee82>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    print(f'Loss: {loss:.8f}')\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Making predictions\n",
    "with torch.no_grad():\n",
    "    y_val = model(categorical_test_data, numerical_test_data)\n",
    "    loss = loss_function(y_val, test_outputs)\n",
    "print(f'Loss: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-59c0b534eda3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_val' is not defined"
     ]
    }
   ],
   "source": [
    "print(y_val[:5])\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "print(y_val[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 225 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  215  216  217  218  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   219  220  221  222  223  224  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  1.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 225 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.DataFrame(X_train)\n",
    "df_test = pd.DataFrame(X_test)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/multi-class-classification/47565\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            outp = self.forward(x)\n",
    "            return F.softmax(outp)\n",
    "# add softmax layer: used for multiclass classification    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_out = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(225 , 1000, 51) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  2, 12,  ...,  4,  2,  3])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss_function = nn.BCELoss()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "model_opt = optim.SGD(exact_net.parameters(), lr = 0.02)\n",
    "\n",
    "train_set_X = Variable(torch.from_numpy(X_train)).float()\n",
    "train_set_y = Variable(torch.LongTensor(y_train)).long()\n",
    "test_set_X = Variable(torch.from_numpy(X_test)).float()\n",
    "test_set_y = Variable(torch.LongTensor(y_test)).long()\n",
    "train_set_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_latent_X = data_utils.TensorDataset(train_set_X, train_set_y)\n",
    "te_latent_X = data_utils.TensorDataset(test_set_X,  test_set_y)\n",
    "train_loader_X = torch.utils.data.DataLoader(dataset=tr_latent_X)\n",
    "\n",
    "test_loader_X = torch.utils.data.DataLoader(dataset=te_latent_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data,label) in enumerate(train_loader_X):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(data)\n",
    "        loss = loss_function(out, label)\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader_X.dataset),\n",
    "                100. * batch_idx / len(train_loader_X), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader_X.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6700 (0%)]\tLoss: 3.902277\n",
      "Train Epoch: 1 [100/6700 (1%)]\tLoss: 3.961245\n",
      "Train Epoch: 1 [200/6700 (3%)]\tLoss: 3.937650\n",
      "Train Epoch: 1 [300/6700 (4%)]\tLoss: 3.883697\n",
      "Train Epoch: 1 [400/6700 (6%)]\tLoss: 4.023554\n",
      "Train Epoch: 1 [500/6700 (7%)]\tLoss: 3.977329\n",
      "Train Epoch: 1 [600/6700 (9%)]\tLoss: 3.843026\n",
      "Train Epoch: 1 [700/6700 (10%)]\tLoss: 3.896039\n",
      "Train Epoch: 1 [800/6700 (12%)]\tLoss: 3.889445\n",
      "Train Epoch: 1 [900/6700 (13%)]\tLoss: 3.968712\n",
      "Train Epoch: 1 [1000/6700 (15%)]\tLoss: 3.892568\n",
      "Train Epoch: 1 [1100/6700 (16%)]\tLoss: 3.882560\n",
      "Train Epoch: 1 [1200/6700 (18%)]\tLoss: 4.781768\n",
      "Train Epoch: 1 [1300/6700 (19%)]\tLoss: 3.985796\n",
      "Train Epoch: 1 [1400/6700 (21%)]\tLoss: 3.885725\n",
      "Train Epoch: 1 [1500/6700 (22%)]\tLoss: 3.866324\n",
      "Train Epoch: 1 [1600/6700 (24%)]\tLoss: 4.468028\n",
      "Train Epoch: 1 [1700/6700 (25%)]\tLoss: 3.887999\n",
      "Train Epoch: 1 [1800/6700 (27%)]\tLoss: 3.968442\n",
      "Train Epoch: 1 [1900/6700 (28%)]\tLoss: 3.798160\n",
      "Train Epoch: 1 [2000/6700 (30%)]\tLoss: 3.890714\n",
      "Train Epoch: 1 [2100/6700 (31%)]\tLoss: 3.902277\n",
      "Train Epoch: 1 [2200/6700 (33%)]\tLoss: 3.865417\n",
      "Train Epoch: 1 [2300/6700 (34%)]\tLoss: 3.858397\n",
      "Train Epoch: 1 [2400/6700 (36%)]\tLoss: 3.897374\n",
      "Train Epoch: 1 [2500/6700 (37%)]\tLoss: 3.997813\n",
      "Train Epoch: 1 [2600/6700 (39%)]\tLoss: 4.029954\n",
      "Train Epoch: 1 [2700/6700 (40%)]\tLoss: 4.069797\n",
      "Train Epoch: 1 [2800/6700 (42%)]\tLoss: 3.927349\n",
      "Train Epoch: 1 [2900/6700 (43%)]\tLoss: 3.872797\n",
      "Train Epoch: 1 [3000/6700 (45%)]\tLoss: 3.919600\n",
      "Train Epoch: 1 [3100/6700 (46%)]\tLoss: 3.855873\n",
      "Train Epoch: 1 [3200/6700 (48%)]\tLoss: 3.968712\n",
      "Train Epoch: 1 [3300/6700 (49%)]\tLoss: 3.909379\n",
      "Train Epoch: 1 [3400/6700 (51%)]\tLoss: 3.913024\n",
      "Train Epoch: 1 [3500/6700 (52%)]\tLoss: 3.587484\n",
      "Train Epoch: 1 [3600/6700 (54%)]\tLoss: 3.735253\n",
      "Train Epoch: 1 [3700/6700 (55%)]\tLoss: 3.924237\n",
      "Train Epoch: 1 [3800/6700 (57%)]\tLoss: 4.066577\n",
      "Train Epoch: 1 [3900/6700 (58%)]\tLoss: 3.852173\n",
      "Train Epoch: 1 [4000/6700 (60%)]\tLoss: 3.912143\n",
      "Train Epoch: 1 [4100/6700 (61%)]\tLoss: 3.937650\n",
      "Train Epoch: 1 [4200/6700 (63%)]\tLoss: 3.956620\n",
      "Train Epoch: 1 [4300/6700 (64%)]\tLoss: 3.686163\n",
      "Train Epoch: 1 [4400/6700 (66%)]\tLoss: 3.880503\n",
      "Train Epoch: 1 [4500/6700 (67%)]\tLoss: 3.841865\n",
      "Train Epoch: 1 [4600/6700 (69%)]\tLoss: 3.880572\n",
      "Train Epoch: 1 [4700/6700 (70%)]\tLoss: 3.913774\n",
      "Train Epoch: 1 [4800/6700 (72%)]\tLoss: 3.928677\n",
      "Train Epoch: 1 [4900/6700 (73%)]\tLoss: 3.920827\n",
      "Train Epoch: 1 [5000/6700 (75%)]\tLoss: 3.940667\n",
      "Train Epoch: 1 [5100/6700 (76%)]\tLoss: 3.793387\n",
      "Train Epoch: 1 [5200/6700 (78%)]\tLoss: 3.881855\n",
      "Train Epoch: 1 [5300/6700 (79%)]\tLoss: 3.910966\n",
      "Train Epoch: 1 [5400/6700 (81%)]\tLoss: 3.841679\n",
      "Train Epoch: 1 [5500/6700 (82%)]\tLoss: 3.774734\n",
      "Train Epoch: 1 [5600/6700 (84%)]\tLoss: 3.956102\n",
      "Train Epoch: 1 [5700/6700 (85%)]\tLoss: 3.780904\n",
      "Train Epoch: 1 [5800/6700 (87%)]\tLoss: 3.931800\n",
      "Train Epoch: 1 [5900/6700 (88%)]\tLoss: 3.901943\n",
      "Train Epoch: 1 [6000/6700 (90%)]\tLoss: 3.920796\n",
      "Train Epoch: 1 [6100/6700 (91%)]\tLoss: 3.955498\n",
      "Train Epoch: 1 [6200/6700 (93%)]\tLoss: 3.920931\n",
      "Train Epoch: 1 [6300/6700 (94%)]\tLoss: 4.009869\n",
      "Train Epoch: 1 [6400/6700 (96%)]\tLoss: 3.989979\n",
      "Train Epoch: 1 [6500/6700 (97%)]\tLoss: 3.977094\n",
      "Train Epoch: 1 [6600/6700 (99%)]\tLoss: 3.897342\n",
      "====> Epoch: 1 Average loss: 3.9122\n",
      "Train Epoch: 2 [0/6700 (0%)]\tLoss: 3.902277\n",
      "Train Epoch: 2 [100/6700 (1%)]\tLoss: 3.961245\n",
      "Train Epoch: 2 [200/6700 (3%)]\tLoss: 3.937650\n",
      "Train Epoch: 2 [300/6700 (4%)]\tLoss: 3.883697\n",
      "Train Epoch: 2 [400/6700 (6%)]\tLoss: 4.023554\n",
      "Train Epoch: 2 [500/6700 (7%)]\tLoss: 3.977329\n",
      "Train Epoch: 2 [600/6700 (9%)]\tLoss: 3.843026\n",
      "Train Epoch: 2 [700/6700 (10%)]\tLoss: 3.896039\n",
      "Train Epoch: 2 [800/6700 (12%)]\tLoss: 3.889445\n",
      "Train Epoch: 2 [900/6700 (13%)]\tLoss: 3.968712\n",
      "Train Epoch: 2 [1000/6700 (15%)]\tLoss: 3.892568\n",
      "Train Epoch: 2 [1100/6700 (16%)]\tLoss: 3.882560\n",
      "Train Epoch: 2 [1200/6700 (18%)]\tLoss: 4.781768\n",
      "Train Epoch: 2 [1300/6700 (19%)]\tLoss: 3.985796\n",
      "Train Epoch: 2 [1400/6700 (21%)]\tLoss: 3.885725\n",
      "Train Epoch: 2 [1500/6700 (22%)]\tLoss: 3.866324\n",
      "Train Epoch: 2 [1600/6700 (24%)]\tLoss: 4.468028\n",
      "Train Epoch: 2 [1700/6700 (25%)]\tLoss: 3.887999\n",
      "Train Epoch: 2 [1800/6700 (27%)]\tLoss: 3.968442\n",
      "Train Epoch: 2 [1900/6700 (28%)]\tLoss: 3.798160\n",
      "Train Epoch: 2 [2000/6700 (30%)]\tLoss: 3.890714\n",
      "Train Epoch: 2 [2100/6700 (31%)]\tLoss: 3.902277\n",
      "Train Epoch: 2 [2200/6700 (33%)]\tLoss: 3.865417\n",
      "Train Epoch: 2 [2300/6700 (34%)]\tLoss: 3.858397\n",
      "Train Epoch: 2 [2400/6700 (36%)]\tLoss: 3.897374\n",
      "Train Epoch: 2 [2500/6700 (37%)]\tLoss: 3.997813\n",
      "Train Epoch: 2 [2600/6700 (39%)]\tLoss: 4.029954\n",
      "Train Epoch: 2 [2700/6700 (40%)]\tLoss: 4.069797\n",
      "Train Epoch: 2 [2800/6700 (42%)]\tLoss: 3.927349\n",
      "Train Epoch: 2 [2900/6700 (43%)]\tLoss: 3.872797\n",
      "Train Epoch: 2 [3000/6700 (45%)]\tLoss: 3.919600\n",
      "Train Epoch: 2 [3100/6700 (46%)]\tLoss: 3.855873\n",
      "Train Epoch: 2 [3200/6700 (48%)]\tLoss: 3.968712\n",
      "Train Epoch: 2 [3300/6700 (49%)]\tLoss: 3.909379\n",
      "Train Epoch: 2 [3400/6700 (51%)]\tLoss: 3.913024\n",
      "Train Epoch: 2 [3500/6700 (52%)]\tLoss: 3.587484\n",
      "Train Epoch: 2 [3600/6700 (54%)]\tLoss: 3.735253\n",
      "Train Epoch: 2 [3700/6700 (55%)]\tLoss: 3.924237\n",
      "Train Epoch: 2 [3800/6700 (57%)]\tLoss: 4.066577\n",
      "Train Epoch: 2 [3900/6700 (58%)]\tLoss: 3.852173\n",
      "Train Epoch: 2 [4000/6700 (60%)]\tLoss: 3.912143\n",
      "Train Epoch: 2 [4100/6700 (61%)]\tLoss: 3.937650\n",
      "Train Epoch: 2 [4200/6700 (63%)]\tLoss: 3.956620\n",
      "Train Epoch: 2 [4300/6700 (64%)]\tLoss: 3.686163\n",
      "Train Epoch: 2 [4400/6700 (66%)]\tLoss: 3.880503\n",
      "Train Epoch: 2 [4500/6700 (67%)]\tLoss: 3.841865\n",
      "Train Epoch: 2 [4600/6700 (69%)]\tLoss: 3.880572\n",
      "Train Epoch: 2 [4700/6700 (70%)]\tLoss: 3.913774\n",
      "Train Epoch: 2 [4800/6700 (72%)]\tLoss: 3.928677\n",
      "Train Epoch: 2 [4900/6700 (73%)]\tLoss: 3.920827\n",
      "Train Epoch: 2 [5000/6700 (75%)]\tLoss: 3.940667\n",
      "Train Epoch: 2 [5100/6700 (76%)]\tLoss: 3.793387\n",
      "Train Epoch: 2 [5200/6700 (78%)]\tLoss: 3.881855\n",
      "Train Epoch: 2 [5300/6700 (79%)]\tLoss: 3.910966\n",
      "Train Epoch: 2 [5400/6700 (81%)]\tLoss: 3.841679\n",
      "Train Epoch: 2 [5500/6700 (82%)]\tLoss: 3.774734\n",
      "Train Epoch: 2 [5600/6700 (84%)]\tLoss: 3.956102\n",
      "Train Epoch: 2 [5700/6700 (85%)]\tLoss: 3.780904\n",
      "Train Epoch: 2 [5800/6700 (87%)]\tLoss: 3.931800\n",
      "Train Epoch: 2 [5900/6700 (88%)]\tLoss: 3.901943\n",
      "Train Epoch: 2 [6000/6700 (90%)]\tLoss: 3.920796\n",
      "Train Epoch: 2 [6100/6700 (91%)]\tLoss: 3.955498\n",
      "Train Epoch: 2 [6200/6700 (93%)]\tLoss: 3.920931\n",
      "Train Epoch: 2 [6300/6700 (94%)]\tLoss: 4.009869\n",
      "Train Epoch: 2 [6400/6700 (96%)]\tLoss: 3.989979\n",
      "Train Epoch: 2 [6500/6700 (97%)]\tLoss: 3.977094\n",
      "Train Epoch: 2 [6600/6700 (99%)]\tLoss: 3.897342\n",
      "====> Epoch: 2 Average loss: 3.9122\n",
      "Train Epoch: 3 [0/6700 (0%)]\tLoss: 3.902277\n",
      "Train Epoch: 3 [100/6700 (1%)]\tLoss: 3.961245\n",
      "Train Epoch: 3 [200/6700 (3%)]\tLoss: 3.937650\n",
      "Train Epoch: 3 [300/6700 (4%)]\tLoss: 3.883697\n",
      "Train Epoch: 3 [400/6700 (6%)]\tLoss: 4.023554\n",
      "Train Epoch: 3 [500/6700 (7%)]\tLoss: 3.977329\n",
      "Train Epoch: 3 [600/6700 (9%)]\tLoss: 3.843026\n",
      "Train Epoch: 3 [700/6700 (10%)]\tLoss: 3.896039\n",
      "Train Epoch: 3 [800/6700 (12%)]\tLoss: 3.889445\n",
      "Train Epoch: 3 [900/6700 (13%)]\tLoss: 3.968712\n",
      "Train Epoch: 3 [1000/6700 (15%)]\tLoss: 3.892568\n",
      "Train Epoch: 3 [1100/6700 (16%)]\tLoss: 3.882560\n",
      "Train Epoch: 3 [1200/6700 (18%)]\tLoss: 4.781768\n",
      "Train Epoch: 3 [1300/6700 (19%)]\tLoss: 3.985796\n",
      "Train Epoch: 3 [1400/6700 (21%)]\tLoss: 3.885725\n",
      "Train Epoch: 3 [1500/6700 (22%)]\tLoss: 3.866324\n",
      "Train Epoch: 3 [1600/6700 (24%)]\tLoss: 4.468028\n",
      "Train Epoch: 3 [1700/6700 (25%)]\tLoss: 3.887999\n",
      "Train Epoch: 3 [1800/6700 (27%)]\tLoss: 3.968442\n",
      "Train Epoch: 3 [1900/6700 (28%)]\tLoss: 3.798160\n",
      "Train Epoch: 3 [2000/6700 (30%)]\tLoss: 3.890714\n",
      "Train Epoch: 3 [2100/6700 (31%)]\tLoss: 3.902277\n",
      "Train Epoch: 3 [2200/6700 (33%)]\tLoss: 3.865417\n",
      "Train Epoch: 3 [2300/6700 (34%)]\tLoss: 3.858397\n",
      "Train Epoch: 3 [2400/6700 (36%)]\tLoss: 3.897374\n",
      "Train Epoch: 3 [2500/6700 (37%)]\tLoss: 3.997813\n",
      "Train Epoch: 3 [2600/6700 (39%)]\tLoss: 4.029954\n",
      "Train Epoch: 3 [2700/6700 (40%)]\tLoss: 4.069797\n",
      "Train Epoch: 3 [2800/6700 (42%)]\tLoss: 3.927349\n",
      "Train Epoch: 3 [2900/6700 (43%)]\tLoss: 3.872797\n",
      "Train Epoch: 3 [3000/6700 (45%)]\tLoss: 3.919600\n",
      "Train Epoch: 3 [3100/6700 (46%)]\tLoss: 3.855873\n",
      "Train Epoch: 3 [3200/6700 (48%)]\tLoss: 3.968712\n",
      "Train Epoch: 3 [3300/6700 (49%)]\tLoss: 3.909379\n",
      "Train Epoch: 3 [3400/6700 (51%)]\tLoss: 3.913024\n",
      "Train Epoch: 3 [3500/6700 (52%)]\tLoss: 3.587484\n",
      "Train Epoch: 3 [3600/6700 (54%)]\tLoss: 3.735253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [3700/6700 (55%)]\tLoss: 3.924237\n",
      "Train Epoch: 3 [3800/6700 (57%)]\tLoss: 4.066577\n",
      "Train Epoch: 3 [3900/6700 (58%)]\tLoss: 3.852173\n",
      "Train Epoch: 3 [4000/6700 (60%)]\tLoss: 3.912143\n",
      "Train Epoch: 3 [4100/6700 (61%)]\tLoss: 3.937650\n",
      "Train Epoch: 3 [4200/6700 (63%)]\tLoss: 3.956620\n",
      "Train Epoch: 3 [4300/6700 (64%)]\tLoss: 3.686163\n",
      "Train Epoch: 3 [4400/6700 (66%)]\tLoss: 3.880503\n",
      "Train Epoch: 3 [4500/6700 (67%)]\tLoss: 3.841865\n",
      "Train Epoch: 3 [4600/6700 (69%)]\tLoss: 3.880572\n",
      "Train Epoch: 3 [4700/6700 (70%)]\tLoss: 3.913774\n",
      "Train Epoch: 3 [4800/6700 (72%)]\tLoss: 3.928677\n",
      "Train Epoch: 3 [4900/6700 (73%)]\tLoss: 3.920827\n",
      "Train Epoch: 3 [5000/6700 (75%)]\tLoss: 3.940667\n",
      "Train Epoch: 3 [5100/6700 (76%)]\tLoss: 3.793387\n",
      "Train Epoch: 3 [5200/6700 (78%)]\tLoss: 3.881855\n",
      "Train Epoch: 3 [5300/6700 (79%)]\tLoss: 3.910966\n",
      "Train Epoch: 3 [5400/6700 (81%)]\tLoss: 3.841679\n",
      "Train Epoch: 3 [5500/6700 (82%)]\tLoss: 3.774734\n",
      "Train Epoch: 3 [5600/6700 (84%)]\tLoss: 3.956102\n",
      "Train Epoch: 3 [5700/6700 (85%)]\tLoss: 3.780904\n",
      "Train Epoch: 3 [5800/6700 (87%)]\tLoss: 3.931800\n",
      "Train Epoch: 3 [5900/6700 (88%)]\tLoss: 3.901943\n",
      "Train Epoch: 3 [6000/6700 (90%)]\tLoss: 3.920796\n",
      "Train Epoch: 3 [6100/6700 (91%)]\tLoss: 3.955498\n",
      "Train Epoch: 3 [6200/6700 (93%)]\tLoss: 3.920931\n",
      "Train Epoch: 3 [6300/6700 (94%)]\tLoss: 4.009869\n",
      "Train Epoch: 3 [6400/6700 (96%)]\tLoss: 3.989979\n",
      "Train Epoch: 3 [6500/6700 (97%)]\tLoss: 3.977094\n",
      "Train Epoch: 3 [6600/6700 (99%)]\tLoss: 3.897342\n",
      "====> Epoch: 3 Average loss: 3.9122\n",
      "Train Epoch: 4 [0/6700 (0%)]\tLoss: 3.902277\n",
      "Train Epoch: 4 [100/6700 (1%)]\tLoss: 3.961245\n",
      "Train Epoch: 4 [200/6700 (3%)]\tLoss: 3.937650\n",
      "Train Epoch: 4 [300/6700 (4%)]\tLoss: 3.883697\n",
      "Train Epoch: 4 [400/6700 (6%)]\tLoss: 4.023554\n",
      "Train Epoch: 4 [500/6700 (7%)]\tLoss: 3.977329\n",
      "Train Epoch: 4 [600/6700 (9%)]\tLoss: 3.843026\n",
      "Train Epoch: 4 [700/6700 (10%)]\tLoss: 3.896039\n",
      "Train Epoch: 4 [800/6700 (12%)]\tLoss: 3.889445\n",
      "Train Epoch: 4 [900/6700 (13%)]\tLoss: 3.968712\n",
      "Train Epoch: 4 [1000/6700 (15%)]\tLoss: 3.892568\n",
      "Train Epoch: 4 [1100/6700 (16%)]\tLoss: 3.882560\n",
      "Train Epoch: 4 [1200/6700 (18%)]\tLoss: 4.781768\n",
      "Train Epoch: 4 [1300/6700 (19%)]\tLoss: 3.985796\n",
      "Train Epoch: 4 [1400/6700 (21%)]\tLoss: 3.885725\n",
      "Train Epoch: 4 [1500/6700 (22%)]\tLoss: 3.866324\n",
      "Train Epoch: 4 [1600/6700 (24%)]\tLoss: 4.468028\n",
      "Train Epoch: 4 [1700/6700 (25%)]\tLoss: 3.887999\n",
      "Train Epoch: 4 [1800/6700 (27%)]\tLoss: 3.968442\n",
      "Train Epoch: 4 [1900/6700 (28%)]\tLoss: 3.798160\n",
      "Train Epoch: 4 [2000/6700 (30%)]\tLoss: 3.890714\n",
      "Train Epoch: 4 [2100/6700 (31%)]\tLoss: 3.902277\n",
      "Train Epoch: 4 [2200/6700 (33%)]\tLoss: 3.865417\n",
      "Train Epoch: 4 [2300/6700 (34%)]\tLoss: 3.858397\n",
      "Train Epoch: 4 [2400/6700 (36%)]\tLoss: 3.897374\n",
      "Train Epoch: 4 [2500/6700 (37%)]\tLoss: 3.997813\n",
      "Train Epoch: 4 [2600/6700 (39%)]\tLoss: 4.029954\n",
      "Train Epoch: 4 [2700/6700 (40%)]\tLoss: 4.069797\n",
      "Train Epoch: 4 [2800/6700 (42%)]\tLoss: 3.927349\n",
      "Train Epoch: 4 [2900/6700 (43%)]\tLoss: 3.872797\n",
      "Train Epoch: 4 [3000/6700 (45%)]\tLoss: 3.919600\n",
      "Train Epoch: 4 [3100/6700 (46%)]\tLoss: 3.855873\n",
      "Train Epoch: 4 [3200/6700 (48%)]\tLoss: 3.968712\n",
      "Train Epoch: 4 [3300/6700 (49%)]\tLoss: 3.909379\n",
      "Train Epoch: 4 [3400/6700 (51%)]\tLoss: 3.913024\n",
      "Train Epoch: 4 [3500/6700 (52%)]\tLoss: 3.587484\n",
      "Train Epoch: 4 [3600/6700 (54%)]\tLoss: 3.735253\n",
      "Train Epoch: 4 [3700/6700 (55%)]\tLoss: 3.924237\n",
      "Train Epoch: 4 [3800/6700 (57%)]\tLoss: 4.066577\n",
      "Train Epoch: 4 [3900/6700 (58%)]\tLoss: 3.852173\n",
      "Train Epoch: 4 [4000/6700 (60%)]\tLoss: 3.912143\n",
      "Train Epoch: 4 [4100/6700 (61%)]\tLoss: 3.937650\n",
      "Train Epoch: 4 [4200/6700 (63%)]\tLoss: 3.956620\n",
      "Train Epoch: 4 [4300/6700 (64%)]\tLoss: 3.686163\n",
      "Train Epoch: 4 [4400/6700 (66%)]\tLoss: 3.880503\n",
      "Train Epoch: 4 [4500/6700 (67%)]\tLoss: 3.841865\n",
      "Train Epoch: 4 [4600/6700 (69%)]\tLoss: 3.880572\n",
      "Train Epoch: 4 [4700/6700 (70%)]\tLoss: 3.913774\n",
      "Train Epoch: 4 [4800/6700 (72%)]\tLoss: 3.928677\n",
      "Train Epoch: 4 [4900/6700 (73%)]\tLoss: 3.920827\n",
      "Train Epoch: 4 [5000/6700 (75%)]\tLoss: 3.940667\n",
      "Train Epoch: 4 [5100/6700 (76%)]\tLoss: 3.793387\n",
      "Train Epoch: 4 [5200/6700 (78%)]\tLoss: 3.881855\n",
      "Train Epoch: 4 [5300/6700 (79%)]\tLoss: 3.910966\n",
      "Train Epoch: 4 [5400/6700 (81%)]\tLoss: 3.841679\n",
      "Train Epoch: 4 [5500/6700 (82%)]\tLoss: 3.774734\n",
      "Train Epoch: 4 [5600/6700 (84%)]\tLoss: 3.956102\n",
      "Train Epoch: 4 [5700/6700 (85%)]\tLoss: 3.780904\n",
      "Train Epoch: 4 [5800/6700 (87%)]\tLoss: 3.931800\n",
      "Train Epoch: 4 [5900/6700 (88%)]\tLoss: 3.901943\n",
      "Train Epoch: 4 [6000/6700 (90%)]\tLoss: 3.920796\n",
      "Train Epoch: 4 [6100/6700 (91%)]\tLoss: 3.955498\n",
      "Train Epoch: 4 [6200/6700 (93%)]\tLoss: 3.920931\n",
      "Train Epoch: 4 [6300/6700 (94%)]\tLoss: 4.009869\n",
      "Train Epoch: 4 [6400/6700 (96%)]\tLoss: 3.989979\n",
      "Train Epoch: 4 [6500/6700 (97%)]\tLoss: 3.977094\n",
      "Train Epoch: 4 [6600/6700 (99%)]\tLoss: 3.897342\n",
      "====> Epoch: 4 Average loss: 3.9122\n",
      "Train Epoch: 5 [0/6700 (0%)]\tLoss: 3.902277\n",
      "Train Epoch: 5 [100/6700 (1%)]\tLoss: 3.961245\n",
      "Train Epoch: 5 [200/6700 (3%)]\tLoss: 3.937650\n",
      "Train Epoch: 5 [300/6700 (4%)]\tLoss: 3.883697\n",
      "Train Epoch: 5 [400/6700 (6%)]\tLoss: 4.023554\n",
      "Train Epoch: 5 [500/6700 (7%)]\tLoss: 3.977329\n",
      "Train Epoch: 5 [600/6700 (9%)]\tLoss: 3.843026\n",
      "Train Epoch: 5 [700/6700 (10%)]\tLoss: 3.896039\n",
      "Train Epoch: 5 [800/6700 (12%)]\tLoss: 3.889445\n",
      "Train Epoch: 5 [900/6700 (13%)]\tLoss: 3.968712\n",
      "Train Epoch: 5 [1000/6700 (15%)]\tLoss: 3.892568\n",
      "Train Epoch: 5 [1100/6700 (16%)]\tLoss: 3.882560\n",
      "Train Epoch: 5 [1200/6700 (18%)]\tLoss: 4.781768\n",
      "Train Epoch: 5 [1300/6700 (19%)]\tLoss: 3.985796\n",
      "Train Epoch: 5 [1400/6700 (21%)]\tLoss: 3.885725\n",
      "Train Epoch: 5 [1500/6700 (22%)]\tLoss: 3.866324\n",
      "Train Epoch: 5 [1600/6700 (24%)]\tLoss: 4.468028\n",
      "Train Epoch: 5 [1700/6700 (25%)]\tLoss: 3.887999\n",
      "Train Epoch: 5 [1800/6700 (27%)]\tLoss: 3.968442\n",
      "Train Epoch: 5 [1900/6700 (28%)]\tLoss: 3.798160\n",
      "Train Epoch: 5 [2000/6700 (30%)]\tLoss: 3.890714\n",
      "Train Epoch: 5 [2100/6700 (31%)]\tLoss: 3.902277\n",
      "Train Epoch: 5 [2200/6700 (33%)]\tLoss: 3.865417\n",
      "Train Epoch: 5 [2300/6700 (34%)]\tLoss: 3.858397\n",
      "Train Epoch: 5 [2400/6700 (36%)]\tLoss: 3.897374\n",
      "Train Epoch: 5 [2500/6700 (37%)]\tLoss: 3.997813\n",
      "Train Epoch: 5 [2600/6700 (39%)]\tLoss: 4.029954\n",
      "Train Epoch: 5 [2700/6700 (40%)]\tLoss: 4.069797\n",
      "Train Epoch: 5 [2800/6700 (42%)]\tLoss: 3.927349\n",
      "Train Epoch: 5 [2900/6700 (43%)]\tLoss: 3.872797\n",
      "Train Epoch: 5 [3000/6700 (45%)]\tLoss: 3.919600\n",
      "Train Epoch: 5 [3100/6700 (46%)]\tLoss: 3.855873\n",
      "Train Epoch: 5 [3200/6700 (48%)]\tLoss: 3.968712\n",
      "Train Epoch: 5 [3300/6700 (49%)]\tLoss: 3.909379\n",
      "Train Epoch: 5 [3400/6700 (51%)]\tLoss: 3.913024\n",
      "Train Epoch: 5 [3500/6700 (52%)]\tLoss: 3.587484\n",
      "Train Epoch: 5 [3600/6700 (54%)]\tLoss: 3.735253\n",
      "Train Epoch: 5 [3700/6700 (55%)]\tLoss: 3.924237\n",
      "Train Epoch: 5 [3800/6700 (57%)]\tLoss: 4.066577\n",
      "Train Epoch: 5 [3900/6700 (58%)]\tLoss: 3.852173\n",
      "Train Epoch: 5 [4000/6700 (60%)]\tLoss: 3.912143\n",
      "Train Epoch: 5 [4100/6700 (61%)]\tLoss: 3.937650\n",
      "Train Epoch: 5 [4200/6700 (63%)]\tLoss: 3.956620\n",
      "Train Epoch: 5 [4300/6700 (64%)]\tLoss: 3.686163\n",
      "Train Epoch: 5 [4400/6700 (66%)]\tLoss: 3.880503\n",
      "Train Epoch: 5 [4500/6700 (67%)]\tLoss: 3.841865\n",
      "Train Epoch: 5 [4600/6700 (69%)]\tLoss: 3.880572\n",
      "Train Epoch: 5 [4700/6700 (70%)]\tLoss: 3.913774\n",
      "Train Epoch: 5 [4800/6700 (72%)]\tLoss: 3.928677\n",
      "Train Epoch: 5 [4900/6700 (73%)]\tLoss: 3.920827\n",
      "Train Epoch: 5 [5000/6700 (75%)]\tLoss: 3.940667\n",
      "Train Epoch: 5 [5100/6700 (76%)]\tLoss: 3.793387\n",
      "Train Epoch: 5 [5200/6700 (78%)]\tLoss: 3.881855\n",
      "Train Epoch: 5 [5300/6700 (79%)]\tLoss: 3.910966\n",
      "Train Epoch: 5 [5400/6700 (81%)]\tLoss: 3.841679\n",
      "Train Epoch: 5 [5500/6700 (82%)]\tLoss: 3.774734\n",
      "Train Epoch: 5 [5600/6700 (84%)]\tLoss: 3.956102\n",
      "Train Epoch: 5 [5700/6700 (85%)]\tLoss: 3.780904\n",
      "Train Epoch: 5 [5800/6700 (87%)]\tLoss: 3.931800\n",
      "Train Epoch: 5 [5900/6700 (88%)]\tLoss: 3.901943\n",
      "Train Epoch: 5 [6000/6700 (90%)]\tLoss: 3.920796\n",
      "Train Epoch: 5 [6100/6700 (91%)]\tLoss: 3.955498\n",
      "Train Epoch: 5 [6200/6700 (93%)]\tLoss: 3.920931\n",
      "Train Epoch: 5 [6300/6700 (94%)]\tLoss: 4.009869\n",
      "Train Epoch: 5 [6400/6700 (96%)]\tLoss: 3.989979\n",
      "Train Epoch: 5 [6500/6700 (97%)]\tLoss: 3.977094\n",
      "Train Epoch: 5 [6600/6700 (99%)]\tLoss: 3.897342\n",
      "====> Epoch: 5 Average loss: 3.9122\n",
      "Train Epoch: 6 [0/6700 (0%)]\tLoss: 3.902277\n",
      "Train Epoch: 6 [100/6700 (1%)]\tLoss: 3.961245\n",
      "Train Epoch: 6 [200/6700 (3%)]\tLoss: 3.937650\n",
      "Train Epoch: 6 [300/6700 (4%)]\tLoss: 3.883697\n",
      "Train Epoch: 6 [400/6700 (6%)]\tLoss: 4.023554\n",
      "Train Epoch: 6 [500/6700 (7%)]\tLoss: 3.977329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [600/6700 (9%)]\tLoss: 3.843026\n",
      "Train Epoch: 6 [700/6700 (10%)]\tLoss: 3.896039\n",
      "Train Epoch: 6 [800/6700 (12%)]\tLoss: 3.889445\n",
      "Train Epoch: 6 [900/6700 (13%)]\tLoss: 3.968712\n",
      "Train Epoch: 6 [1000/6700 (15%)]\tLoss: 3.892568\n",
      "Train Epoch: 6 [1100/6700 (16%)]\tLoss: 3.882560\n",
      "Train Epoch: 6 [1200/6700 (18%)]\tLoss: 4.781768\n",
      "Train Epoch: 6 [1300/6700 (19%)]\tLoss: 3.985796\n",
      "Train Epoch: 6 [1400/6700 (21%)]\tLoss: 3.885725\n",
      "Train Epoch: 6 [1500/6700 (22%)]\tLoss: 3.866324\n",
      "Train Epoch: 6 [1600/6700 (24%)]\tLoss: 4.468028\n",
      "Train Epoch: 6 [1700/6700 (25%)]\tLoss: 3.887999\n",
      "Train Epoch: 6 [1800/6700 (27%)]\tLoss: 3.968442\n",
      "Train Epoch: 6 [1900/6700 (28%)]\tLoss: 3.798160\n",
      "Train Epoch: 6 [2000/6700 (30%)]\tLoss: 3.890714\n",
      "Train Epoch: 6 [2100/6700 (31%)]\tLoss: 3.902277\n",
      "Train Epoch: 6 [2200/6700 (33%)]\tLoss: 3.865417\n",
      "Train Epoch: 6 [2300/6700 (34%)]\tLoss: 3.858397\n",
      "Train Epoch: 6 [2400/6700 (36%)]\tLoss: 3.897374\n",
      "Train Epoch: 6 [2500/6700 (37%)]\tLoss: 3.997813\n",
      "Train Epoch: 6 [2600/6700 (39%)]\tLoss: 4.029954\n",
      "Train Epoch: 6 [2700/6700 (40%)]\tLoss: 4.069797\n",
      "Train Epoch: 6 [2800/6700 (42%)]\tLoss: 3.927349\n",
      "Train Epoch: 6 [2900/6700 (43%)]\tLoss: 3.872797\n",
      "Train Epoch: 6 [3000/6700 (45%)]\tLoss: 3.919600\n",
      "Train Epoch: 6 [3100/6700 (46%)]\tLoss: 3.855873\n",
      "Train Epoch: 6 [3200/6700 (48%)]\tLoss: 3.968712\n",
      "Train Epoch: 6 [3300/6700 (49%)]\tLoss: 3.909379\n",
      "Train Epoch: 6 [3400/6700 (51%)]\tLoss: 3.913024\n",
      "Train Epoch: 6 [3500/6700 (52%)]\tLoss: 3.587484\n",
      "Train Epoch: 6 [3600/6700 (54%)]\tLoss: 3.735253\n",
      "Train Epoch: 6 [3700/6700 (55%)]\tLoss: 3.924237\n",
      "Train Epoch: 6 [3800/6700 (57%)]\tLoss: 4.066577\n",
      "Train Epoch: 6 [3900/6700 (58%)]\tLoss: 3.852173\n",
      "Train Epoch: 6 [4000/6700 (60%)]\tLoss: 3.912143\n",
      "Train Epoch: 6 [4100/6700 (61%)]\tLoss: 3.937650\n",
      "Train Epoch: 6 [4200/6700 (63%)]\tLoss: 3.956620\n",
      "Train Epoch: 6 [4300/6700 (64%)]\tLoss: 3.686163\n",
      "Train Epoch: 6 [4400/6700 (66%)]\tLoss: 3.880503\n",
      "Train Epoch: 6 [4500/6700 (67%)]\tLoss: 3.841865\n",
      "Train Epoch: 6 [4600/6700 (69%)]\tLoss: 3.880572\n",
      "Train Epoch: 6 [4700/6700 (70%)]\tLoss: 3.913774\n",
      "Train Epoch: 6 [4800/6700 (72%)]\tLoss: 3.928677\n",
      "Train Epoch: 6 [4900/6700 (73%)]\tLoss: 3.920827\n",
      "Train Epoch: 6 [5000/6700 (75%)]\tLoss: 3.940667\n",
      "Train Epoch: 6 [5100/6700 (76%)]\tLoss: 3.793387\n",
      "Train Epoch: 6 [5200/6700 (78%)]\tLoss: 3.881855\n",
      "Train Epoch: 6 [5300/6700 (79%)]\tLoss: 3.910966\n",
      "Train Epoch: 6 [5400/6700 (81%)]\tLoss: 3.841679\n",
      "Train Epoch: 6 [5500/6700 (82%)]\tLoss: 3.774734\n",
      "Train Epoch: 6 [5600/6700 (84%)]\tLoss: 3.956102\n",
      "Train Epoch: 6 [5700/6700 (85%)]\tLoss: 3.780904\n",
      "Train Epoch: 6 [5800/6700 (87%)]\tLoss: 3.931800\n",
      "Train Epoch: 6 [5900/6700 (88%)]\tLoss: 3.901943\n",
      "Train Epoch: 6 [6000/6700 (90%)]\tLoss: 3.920796\n",
      "Train Epoch: 6 [6100/6700 (91%)]\tLoss: 3.955498\n",
      "Train Epoch: 6 [6200/6700 (93%)]\tLoss: 3.920931\n",
      "Train Epoch: 6 [6300/6700 (94%)]\tLoss: 4.009869\n",
      "Train Epoch: 6 [6400/6700 (96%)]\tLoss: 3.989979\n",
      "Train Epoch: 6 [6500/6700 (97%)]\tLoss: 3.977094\n",
      "Train Epoch: 6 [6600/6700 (99%)]\tLoss: 3.897342\n",
      "====> Epoch: 6 Average loss: 3.9122\n",
      "Train Epoch: 7 [0/6700 (0%)]\tLoss: 3.902277\n",
      "Train Epoch: 7 [100/6700 (1%)]\tLoss: 3.961245\n",
      "Train Epoch: 7 [200/6700 (3%)]\tLoss: 3.937650\n",
      "Train Epoch: 7 [300/6700 (4%)]\tLoss: 3.883697\n",
      "Train Epoch: 7 [400/6700 (6%)]\tLoss: 4.023554\n",
      "Train Epoch: 7 [500/6700 (7%)]\tLoss: 3.977329\n",
      "Train Epoch: 7 [600/6700 (9%)]\tLoss: 3.843026\n",
      "Train Epoch: 7 [700/6700 (10%)]\tLoss: 3.896039\n",
      "Train Epoch: 7 [800/6700 (12%)]\tLoss: 3.889445\n",
      "Train Epoch: 7 [900/6700 (13%)]\tLoss: 3.968712\n",
      "Train Epoch: 7 [1000/6700 (15%)]\tLoss: 3.892568\n",
      "Train Epoch: 7 [1100/6700 (16%)]\tLoss: 3.882560\n",
      "Train Epoch: 7 [1200/6700 (18%)]\tLoss: 4.781768\n",
      "Train Epoch: 7 [1300/6700 (19%)]\tLoss: 3.985796\n",
      "Train Epoch: 7 [1400/6700 (21%)]\tLoss: 3.885725\n",
      "Train Epoch: 7 [1500/6700 (22%)]\tLoss: 3.866324\n",
      "Train Epoch: 7 [1600/6700 (24%)]\tLoss: 4.468028\n",
      "Train Epoch: 7 [1700/6700 (25%)]\tLoss: 3.887999\n",
      "Train Epoch: 7 [1800/6700 (27%)]\tLoss: 3.968442\n",
      "Train Epoch: 7 [1900/6700 (28%)]\tLoss: 3.798160\n",
      "Train Epoch: 7 [2000/6700 (30%)]\tLoss: 3.890714\n",
      "Train Epoch: 7 [2100/6700 (31%)]\tLoss: 3.902277\n",
      "Train Epoch: 7 [2200/6700 (33%)]\tLoss: 3.865417\n",
      "Train Epoch: 7 [2300/6700 (34%)]\tLoss: 3.858397\n",
      "Train Epoch: 7 [2400/6700 (36%)]\tLoss: 3.897374\n",
      "Train Epoch: 7 [2500/6700 (37%)]\tLoss: 3.997813\n",
      "Train Epoch: 7 [2600/6700 (39%)]\tLoss: 4.029954\n",
      "Train Epoch: 7 [2700/6700 (40%)]\tLoss: 4.069797\n",
      "Train Epoch: 7 [2800/6700 (42%)]\tLoss: 3.927349\n",
      "Train Epoch: 7 [2900/6700 (43%)]\tLoss: 3.872797\n",
      "Train Epoch: 7 [3000/6700 (45%)]\tLoss: 3.919600\n",
      "Train Epoch: 7 [3100/6700 (46%)]\tLoss: 3.855873\n",
      "Train Epoch: 7 [3200/6700 (48%)]\tLoss: 3.968712\n",
      "Train Epoch: 7 [3300/6700 (49%)]\tLoss: 3.909379\n",
      "Train Epoch: 7 [3400/6700 (51%)]\tLoss: 3.913024\n",
      "Train Epoch: 7 [3500/6700 (52%)]\tLoss: 3.587484\n",
      "Train Epoch: 7 [3600/6700 (54%)]\tLoss: 3.735253\n",
      "Train Epoch: 7 [3700/6700 (55%)]\tLoss: 3.924237\n",
      "Train Epoch: 7 [3800/6700 (57%)]\tLoss: 4.066577\n",
      "Train Epoch: 7 [3900/6700 (58%)]\tLoss: 3.852173\n",
      "Train Epoch: 7 [4000/6700 (60%)]\tLoss: 3.912143\n",
      "Train Epoch: 7 [4100/6700 (61%)]\tLoss: 3.937650\n",
      "Train Epoch: 7 [4200/6700 (63%)]\tLoss: 3.956620\n",
      "Train Epoch: 7 [4300/6700 (64%)]\tLoss: 3.686163\n",
      "Train Epoch: 7 [4400/6700 (66%)]\tLoss: 3.880503\n",
      "Train Epoch: 7 [4500/6700 (67%)]\tLoss: 3.841865\n",
      "Train Epoch: 7 [4600/6700 (69%)]\tLoss: 3.880572\n",
      "Train Epoch: 7 [4700/6700 (70%)]\tLoss: 3.913774\n",
      "Train Epoch: 7 [4800/6700 (72%)]\tLoss: 3.928677\n",
      "Train Epoch: 7 [4900/6700 (73%)]\tLoss: 3.920827\n",
      "Train Epoch: 7 [5000/6700 (75%)]\tLoss: 3.940667\n",
      "Train Epoch: 7 [5100/6700 (76%)]\tLoss: 3.793387\n",
      "Train Epoch: 7 [5200/6700 (78%)]\tLoss: 3.881855\n",
      "Train Epoch: 7 [5300/6700 (79%)]\tLoss: 3.910966\n",
      "Train Epoch: 7 [5400/6700 (81%)]\tLoss: 3.841679\n",
      "Train Epoch: 7 [5500/6700 (82%)]\tLoss: 3.774734\n",
      "Train Epoch: 7 [5600/6700 (84%)]\tLoss: 3.956102\n",
      "Train Epoch: 7 [5700/6700 (85%)]\tLoss: 3.780904\n",
      "Train Epoch: 7 [5800/6700 (87%)]\tLoss: 3.931800\n",
      "Train Epoch: 7 [5900/6700 (88%)]\tLoss: 3.901943\n",
      "Train Epoch: 7 [6000/6700 (90%)]\tLoss: 3.920796\n",
      "Train Epoch: 7 [6100/6700 (91%)]\tLoss: 3.955498\n",
      "Train Epoch: 7 [6200/6700 (93%)]\tLoss: 3.920931\n",
      "Train Epoch: 7 [6300/6700 (94%)]\tLoss: 4.009869\n",
      "Train Epoch: 7 [6400/6700 (96%)]\tLoss: 3.989979\n",
      "Train Epoch: 7 [6500/6700 (97%)]\tLoss: 3.977094\n",
      "Train Epoch: 7 [6600/6700 (99%)]\tLoss: 3.897342\n",
      "====> Epoch: 7 Average loss: 3.9122\n",
      "Train Epoch: 8 [0/6700 (0%)]\tLoss: 3.902277\n",
      "Train Epoch: 8 [100/6700 (1%)]\tLoss: 3.961245\n",
      "Train Epoch: 8 [200/6700 (3%)]\tLoss: 3.937650\n",
      "Train Epoch: 8 [300/6700 (4%)]\tLoss: 3.883697\n",
      "Train Epoch: 8 [400/6700 (6%)]\tLoss: 4.023554\n",
      "Train Epoch: 8 [500/6700 (7%)]\tLoss: 3.977329\n",
      "Train Epoch: 8 [600/6700 (9%)]\tLoss: 3.843026\n",
      "Train Epoch: 8 [700/6700 (10%)]\tLoss: 3.896039\n",
      "Train Epoch: 8 [800/6700 (12%)]\tLoss: 3.889445\n",
      "Train Epoch: 8 [900/6700 (13%)]\tLoss: 3.968712\n",
      "Train Epoch: 8 [1000/6700 (15%)]\tLoss: 3.892568\n",
      "Train Epoch: 8 [1100/6700 (16%)]\tLoss: 3.882560\n",
      "Train Epoch: 8 [1200/6700 (18%)]\tLoss: 4.781768\n",
      "Train Epoch: 8 [1300/6700 (19%)]\tLoss: 3.985796\n",
      "Train Epoch: 8 [1400/6700 (21%)]\tLoss: 3.885725\n",
      "Train Epoch: 8 [1500/6700 (22%)]\tLoss: 3.866324\n",
      "Train Epoch: 8 [1600/6700 (24%)]\tLoss: 4.468028\n",
      "Train Epoch: 8 [1700/6700 (25%)]\tLoss: 3.887999\n",
      "Train Epoch: 8 [1800/6700 (27%)]\tLoss: 3.968442\n",
      "Train Epoch: 8 [1900/6700 (28%)]\tLoss: 3.798160\n",
      "Train Epoch: 8 [2000/6700 (30%)]\tLoss: 3.890714\n",
      "Train Epoch: 8 [2100/6700 (31%)]\tLoss: 3.902277\n",
      "Train Epoch: 8 [2200/6700 (33%)]\tLoss: 3.865417\n",
      "Train Epoch: 8 [2300/6700 (34%)]\tLoss: 3.858397\n",
      "Train Epoch: 8 [2400/6700 (36%)]\tLoss: 3.897374\n",
      "Train Epoch: 8 [2500/6700 (37%)]\tLoss: 3.997813\n",
      "Train Epoch: 8 [2600/6700 (39%)]\tLoss: 4.029954\n",
      "Train Epoch: 8 [2700/6700 (40%)]\tLoss: 4.069797\n",
      "Train Epoch: 8 [2800/6700 (42%)]\tLoss: 3.927349\n",
      "Train Epoch: 8 [2900/6700 (43%)]\tLoss: 3.872797\n",
      "Train Epoch: 8 [3000/6700 (45%)]\tLoss: 3.919600\n",
      "Train Epoch: 8 [3100/6700 (46%)]\tLoss: 3.855873\n",
      "Train Epoch: 8 [3200/6700 (48%)]\tLoss: 3.968712\n",
      "Train Epoch: 8 [3300/6700 (49%)]\tLoss: 3.909379\n",
      "Train Epoch: 8 [3400/6700 (51%)]\tLoss: 3.913024\n",
      "Train Epoch: 8 [3500/6700 (52%)]\tLoss: 3.587484\n",
      "Train Epoch: 8 [3600/6700 (54%)]\tLoss: 3.735253\n",
      "Train Epoch: 8 [3700/6700 (55%)]\tLoss: 3.924237\n",
      "Train Epoch: 8 [3800/6700 (57%)]\tLoss: 4.066577\n",
      "Train Epoch: 8 [3900/6700 (58%)]\tLoss: 3.852173\n",
      "Train Epoch: 8 [4000/6700 (60%)]\tLoss: 3.912143\n",
      "Train Epoch: 8 [4100/6700 (61%)]\tLoss: 3.937650\n",
      "Train Epoch: 8 [4200/6700 (63%)]\tLoss: 3.956620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [4300/6700 (64%)]\tLoss: 3.686163\n",
      "Train Epoch: 8 [4400/6700 (66%)]\tLoss: 3.880503\n",
      "Train Epoch: 8 [4500/6700 (67%)]\tLoss: 3.841865\n",
      "Train Epoch: 8 [4600/6700 (69%)]\tLoss: 3.880572\n",
      "Train Epoch: 8 [4700/6700 (70%)]\tLoss: 3.913774\n",
      "Train Epoch: 8 [4800/6700 (72%)]\tLoss: 3.928677\n",
      "Train Epoch: 8 [4900/6700 (73%)]\tLoss: 3.920827\n",
      "Train Epoch: 8 [5000/6700 (75%)]\tLoss: 3.940667\n",
      "Train Epoch: 8 [5100/6700 (76%)]\tLoss: 3.793387\n",
      "Train Epoch: 8 [5200/6700 (78%)]\tLoss: 3.881855\n",
      "Train Epoch: 8 [5300/6700 (79%)]\tLoss: 3.910966\n",
      "Train Epoch: 8 [5400/6700 (81%)]\tLoss: 3.841679\n",
      "Train Epoch: 8 [5500/6700 (82%)]\tLoss: 3.774734\n",
      "Train Epoch: 8 [5600/6700 (84%)]\tLoss: 3.956102\n",
      "Train Epoch: 8 [5700/6700 (85%)]\tLoss: 3.780904\n",
      "Train Epoch: 8 [5800/6700 (87%)]\tLoss: 3.931800\n",
      "Train Epoch: 8 [5900/6700 (88%)]\tLoss: 3.901943\n",
      "Train Epoch: 8 [6000/6700 (90%)]\tLoss: 3.920796\n",
      "Train Epoch: 8 [6100/6700 (91%)]\tLoss: 3.955498\n",
      "Train Epoch: 8 [6200/6700 (93%)]\tLoss: 3.920931\n",
      "Train Epoch: 8 [6300/6700 (94%)]\tLoss: 4.009869\n",
      "Train Epoch: 8 [6400/6700 (96%)]\tLoss: 3.989979\n",
      "Train Epoch: 8 [6500/6700 (97%)]\tLoss: 3.977094\n",
      "Train Epoch: 8 [6600/6700 (99%)]\tLoss: 3.897342\n",
      "====> Epoch: 8 Average loss: 3.9122\n",
      "Train Epoch: 9 [0/6700 (0%)]\tLoss: 3.902277\n",
      "Train Epoch: 9 [100/6700 (1%)]\tLoss: 3.961245\n",
      "Train Epoch: 9 [200/6700 (3%)]\tLoss: 3.937650\n",
      "Train Epoch: 9 [300/6700 (4%)]\tLoss: 3.883697\n",
      "Train Epoch: 9 [400/6700 (6%)]\tLoss: 4.023554\n",
      "Train Epoch: 9 [500/6700 (7%)]\tLoss: 3.977329\n",
      "Train Epoch: 9 [600/6700 (9%)]\tLoss: 3.843026\n",
      "Train Epoch: 9 [700/6700 (10%)]\tLoss: 3.896039\n",
      "Train Epoch: 9 [800/6700 (12%)]\tLoss: 3.889445\n",
      "Train Epoch: 9 [900/6700 (13%)]\tLoss: 3.968712\n",
      "Train Epoch: 9 [1000/6700 (15%)]\tLoss: 3.892568\n",
      "Train Epoch: 9 [1100/6700 (16%)]\tLoss: 3.882560\n",
      "Train Epoch: 9 [1200/6700 (18%)]\tLoss: 4.781768\n",
      "Train Epoch: 9 [1300/6700 (19%)]\tLoss: 3.985796\n",
      "Train Epoch: 9 [1400/6700 (21%)]\tLoss: 3.885725\n",
      "Train Epoch: 9 [1500/6700 (22%)]\tLoss: 3.866324\n",
      "Train Epoch: 9 [1600/6700 (24%)]\tLoss: 4.468028\n",
      "Train Epoch: 9 [1700/6700 (25%)]\tLoss: 3.887999\n",
      "Train Epoch: 9 [1800/6700 (27%)]\tLoss: 3.968442\n",
      "Train Epoch: 9 [1900/6700 (28%)]\tLoss: 3.798160\n",
      "Train Epoch: 9 [2000/6700 (30%)]\tLoss: 3.890714\n",
      "Train Epoch: 9 [2100/6700 (31%)]\tLoss: 3.902277\n",
      "Train Epoch: 9 [2200/6700 (33%)]\tLoss: 3.865417\n",
      "Train Epoch: 9 [2300/6700 (34%)]\tLoss: 3.858397\n",
      "Train Epoch: 9 [2400/6700 (36%)]\tLoss: 3.897374\n",
      "Train Epoch: 9 [2500/6700 (37%)]\tLoss: 3.997813\n",
      "Train Epoch: 9 [2600/6700 (39%)]\tLoss: 4.029954\n",
      "Train Epoch: 9 [2700/6700 (40%)]\tLoss: 4.069797\n",
      "Train Epoch: 9 [2800/6700 (42%)]\tLoss: 3.927349\n",
      "Train Epoch: 9 [2900/6700 (43%)]\tLoss: 3.872797\n",
      "Train Epoch: 9 [3000/6700 (45%)]\tLoss: 3.919600\n",
      "Train Epoch: 9 [3100/6700 (46%)]\tLoss: 3.855873\n",
      "Train Epoch: 9 [3200/6700 (48%)]\tLoss: 3.968712\n",
      "Train Epoch: 9 [3300/6700 (49%)]\tLoss: 3.909379\n",
      "Train Epoch: 9 [3400/6700 (51%)]\tLoss: 3.913024\n",
      "Train Epoch: 9 [3500/6700 (52%)]\tLoss: 3.587484\n",
      "Train Epoch: 9 [3600/6700 (54%)]\tLoss: 3.735253\n",
      "Train Epoch: 9 [3700/6700 (55%)]\tLoss: 3.924237\n",
      "Train Epoch: 9 [3800/6700 (57%)]\tLoss: 4.066577\n",
      "Train Epoch: 9 [3900/6700 (58%)]\tLoss: 3.852173\n",
      "Train Epoch: 9 [4000/6700 (60%)]\tLoss: 3.912143\n",
      "Train Epoch: 9 [4100/6700 (61%)]\tLoss: 3.937650\n",
      "Train Epoch: 9 [4200/6700 (63%)]\tLoss: 3.956620\n",
      "Train Epoch: 9 [4300/6700 (64%)]\tLoss: 3.686163\n",
      "Train Epoch: 9 [4400/6700 (66%)]\tLoss: 3.880503\n",
      "Train Epoch: 9 [4500/6700 (67%)]\tLoss: 3.841865\n",
      "Train Epoch: 9 [4600/6700 (69%)]\tLoss: 3.880572\n",
      "Train Epoch: 9 [4700/6700 (70%)]\tLoss: 3.913774\n",
      "Train Epoch: 9 [4800/6700 (72%)]\tLoss: 3.928677\n",
      "Train Epoch: 9 [4900/6700 (73%)]\tLoss: 3.920827\n",
      "Train Epoch: 9 [5000/6700 (75%)]\tLoss: 3.940667\n",
      "Train Epoch: 9 [5100/6700 (76%)]\tLoss: 3.793387\n",
      "Train Epoch: 9 [5200/6700 (78%)]\tLoss: 3.881855\n",
      "Train Epoch: 9 [5300/6700 (79%)]\tLoss: 3.910966\n",
      "Train Epoch: 9 [5400/6700 (81%)]\tLoss: 3.841679\n",
      "Train Epoch: 9 [5500/6700 (82%)]\tLoss: 3.774734\n",
      "Train Epoch: 9 [5600/6700 (84%)]\tLoss: 3.956102\n",
      "Train Epoch: 9 [5700/6700 (85%)]\tLoss: 3.780904\n",
      "Train Epoch: 9 [5800/6700 (87%)]\tLoss: 3.931800\n",
      "Train Epoch: 9 [5900/6700 (88%)]\tLoss: 3.901943\n",
      "Train Epoch: 9 [6000/6700 (90%)]\tLoss: 3.920796\n",
      "Train Epoch: 9 [6100/6700 (91%)]\tLoss: 3.955498\n",
      "Train Epoch: 9 [6200/6700 (93%)]\tLoss: 3.920931\n",
      "Train Epoch: 9 [6300/6700 (94%)]\tLoss: 4.009869\n",
      "Train Epoch: 9 [6400/6700 (96%)]\tLoss: 3.989979\n",
      "Train Epoch: 9 [6500/6700 (97%)]\tLoss: 3.977094\n",
      "Train Epoch: 9 [6600/6700 (99%)]\tLoss: 3.897342\n",
      "====> Epoch: 9 Average loss: 3.9122\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "for epochs in range(epochs):\n",
    "    model_opt.zero_grad()\n",
    "    out = model(train_set_X)\n",
    "    loss = loss_function(out, train_set_y)\n",
    "    loss.backward()\n",
    "    model_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/Vpy35/lib/python3.5/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "ans = model.predict(test_set_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3300, 51])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0192, 0.0226, 0.0191,  ..., 0.0170, 0.0201, 0.0192],\n",
       "        [0.0203, 0.0201, 0.0185,  ..., 0.0197, 0.0203, 0.0193],\n",
       "        [0.0194, 0.0211, 0.0182,  ..., 0.0162, 0.0197, 0.0200],\n",
       "        ...,\n",
       "        [0.0207, 0.0234, 0.0187,  ..., 0.0173, 0.0213, 0.0190],\n",
       "        [0.0198, 0.0209, 0.0200,  ..., 0.0176, 0.0196, 0.0177],\n",
       "        [0.0206, 0.0209, 0.0203,  ..., 0.0161, 0.0206, 0.0195]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_max,ind = torch.max(ans[2245,:],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 2, 3,  ..., 9, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
