# -*- coding: utf-8 -*-
"""proximal_GD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gij1APraqynq5AnawNHBhOYAw4y9Y0nt
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/My Drive/assn1/')
import random
import cs771
from cs771 import helloWorld as hW
from cs771 import utils as ut

# Sir's Code
import numpy as np
from submit import solver

# Find out how much loss is the learnt model incurring?
def getObjValue( X, y, wHat ):
	lassoLoss = np.linalg.norm( wHat, 1 ) + pow( np.linalg.norm( X.dot( wHat ) - y, 2 ), 2 )
	return lassoLoss

# Find out how far is the learnt model from the true one in terms of Euclidean distance
def getModelError( wHat, wAst ):
	return np.linalg.norm( wHat - wAst, 2 )

# Force the learnt model to become sparse and then see how well it approximates the true model
def getSupportError( wHat, wAst, k ):
	# Find the k coordinates where the true model has non-zero values
	idxAst = np.abs( wAst ).argsort()[::-1][:k]
	# Find the k coordinates with largest values (absolute terms) in the learnt model
	idxHat = np.abs( wHat ).argsort()[::-1][:k]
	
	# Set up indicator arrays to find the diff between the two
	# Could have used Python's set difference function here as well
	a = np.zeros_like( wAst )
	a[idxAst] = 1
	b = np.zeros_like( wAst )
	b[idxHat] = 1
	return np.linalg.norm( a - b, 1 )//2

Z = np.loadtxt( "train" )
# wAst = np.loadtxt( "wAstTest" )

y = Z[:,0]
X = Z[:,1:]

# # To avoid unlucky outcomes try running the code several times
# # numTrials = 5

# # Try various timeouts - the timeouts are in seconds
# timeouts = np.array( [0.1, 1, 2, 5] )

# # Try checking for timeout every 10 iterations
# spacing = 10

# result = np.zeros( len( timeouts ), 4 )

# for i in len( timeouts ):
# 	to = timeouts[i]
# 	avgObj = 0
# 	avgDist = 0
# 	avgSupp = 0
# 	avgTime = 0
# 	for t in range( numTrials ):
# 		(w, totTime) = solver( X, y, to, spacing )
# 		avgObj = avgObj + getObjValue( X, y, w )
# 		avgDist = avgDist + getModelError( w, wAst )
# 		avgSupp = avgSupp + getSupportError( w, wAst, k )
# 		avgTime = avgTime + totTime
# 	result[i, 0] = avgObj/numTrials
# 	result[i, 1] = avgDist/numTrials
# 	result[i, 2] = avgSupp/numTrials
# 	result[i, 3] = avgTime/numTrials

# np.savetxt( "result", result, fmt = "%.6f" )

# w initialization
w_0=np.ones(X.shape[1])*2

# Gives objective value
def getObjValue( X, y, wHat ):
    k = 1
    lassoLoss = k*np.linalg.norm( wHat, ord=1 )+pow( np.linalg.norm( X.dot( wHat ) - y, 2 ), 2 )
    return lassoLoss

# Returns projected value
def soft_threshold(w,eta_t):
  alpha=1
  alpha = alpha*eta_t 
  prox = np.zeros_like(w)
  for i in range(len(w)):
      if w[i] > alpha:
        prox[i] = w[i]-alpha
      elif w[i] < -alpha:
        prox[i] = w[i]+alpha
      else:
        prox[i] = 0
  return prox

#Gives subgradient value
def sg(X,y,w,t):
    value=2*((X.T)@(X@w-y))+np.sign(w)
    return value

# Function for subgradient descent
def SubGD(X,y,w):
  tol=.001
  w_f=w
  diff=1000000
  print (getObjValue(X,y,w))
  t=1
  eta_t=1/pow(t,1/2)
  n=X.shape[0]
  while diff>tol:
  
    # g=sg(X,y,w,t)
    # print(t)
    g=sg(X,y,w,t)
    eta_t=.1/pow(t,1/2)
    w_new=soft_threshold(w-eta_t*g,eta_t)
    # print(w_new)
    w_f=np.vstack((w,w_new))
    # w_new=np.mean(w_f,axis=0)
    print (getObjValue(X,y,w_new))
    diff=getObjValue( X, y, w )-getObjValue( X, y, w_new )
    # print(diff)
    a=w
    w=w_new
    t=t+1
  
  return (a,w_f)

(ans,w_final)=SubGD(X,y,w_0)
val=getObjValue(X,y,ans)

ans

ans[abs(ans).argsort()[-20:][::-1]]

abs(ans).argsort()[-20:][::-1]

samp_ans = np.loadtxt( "/content/drive/My Drive/assn1 (1)/assn1/wAstTrain" )
#Getting coordinates of wAstTrain
abs(samp_ans).argsort()[-20:][::-1]

abs(ans).argsort()[-20:][::-1][~np.isin(abs(samp_ans).argsort()[-20:][::-1],np.argsort(abs(ans))[-20:])]

samp_ans[abs(samp_ans).argsort()[-20:][::-1]]

w_zero=np.zeros(1000)
w_zero[abs(ans).argsort()[-20:][::-1]]=ans[abs(ans).argsort()[-20:][::-1]]

getModelError( ans, samp_ans)

w_zero

