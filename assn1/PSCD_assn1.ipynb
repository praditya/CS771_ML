{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cs771 import genSyntheticData as gsd\n",
    "from cs771 import plotData as pd\n",
    "from cs771 import optLib as opt\n",
    "from matplotlib import pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solver( X, y, timeout, spacing ):\n",
    "\t(n, d) = X.shape\n",
    "\tt = 0\n",
    "\ttotTime = 0\n",
    "\t\n",
    "\t# w is the model vector and will get returned once timeout happens\n",
    "\tw = np.zeros( (d,) )\n",
    "\ttic = tm.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = np.loadtxt(\"train\")\n",
    "y = traindata[:,-1]\n",
    "X = traindata[:,0:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Gradient Descent\n",
    "def getCSVMGrad( theta, t ):\n",
    "    w = theta[0:-1]\n",
    "    b = theta[-1]\n",
    "    discriminant = np.multiply( (X.dot( w ) + b), y )\n",
    "    g = np.zeros( (y.size,) )\n",
    "    g[discriminant < 1] = -1\n",
    "    delb = C * g.dot( y )\n",
    "    delw = w + C * (X.T * g).dot( y )\n",
    "    return np.append( delw, delb )\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "def getCSVMSGrad( theta, t ):\n",
    "    w = theta[0:-1]\n",
    "    b = theta[-1]\n",
    "    n = y.size\n",
    "    i = random.randint( 0, n-1 )\n",
    "    x = X[i,:]\n",
    "    discriminant = (x.dot( w ) + b) * y[i]\n",
    "    g = 0\n",
    "    if discriminant < 1:\n",
    "        g = -1\n",
    "    delb = C * n * g * y[i]\n",
    "    delw = w + C * n * (x * g) * y[i]\n",
    "    return np.append( delw, delb )\n",
    "\n",
    "# Mini-batch Stochastic Gradient Descent\n",
    "def getCSVMMBGrad( theta, t ):\n",
    "    w = theta[0:-1]\n",
    "    b = theta[-1]\n",
    "    n = y.size\n",
    "    # Be careful not to ask for more samples than there are training points\n",
    "    # otherwise the sample() routine will throw an exception\n",
    "    B_eff = min( B, n )\n",
    "    samples = random.sample( range(0, n), B_eff )\n",
    "    X_ = X[samples,:]\n",
    "    y_ = y[samples]\n",
    "    discriminant = np.multiply( (X_.dot( w ) + b), y_ )\n",
    "    g = np.zeros( (B_eff,) )\n",
    "    g[discriminant < 1] = -1\n",
    "    delb = C * n/B_eff * g.dot( y_ )\n",
    "    delw = w + C * n/B_eff * (X_.T * g).dot( y_ )\n",
    "    return np.append( delw, delb )\n",
    "\n",
    "# Variable Batch-size Stochastic Gradient Descent\n",
    "def getCSVMVarMBGrad( theta, t ):\n",
    "    w = theta[0:-1]\n",
    "    b = theta[-1]\n",
    "    n = y.size\n",
    "    # Increase the batch size every few iterations -- there are two tuneable hyperparameters here\n",
    "    # How frequently to update the batch size and how much to increase it at each update\n",
    "    B_eff = min( B * int( pow(1.2, t//40) ), n )\n",
    "    samples = random.sample( range(0, n), B_eff )\n",
    "    X_ = X[samples,:]\n",
    "    y_ = y[samples]\n",
    "    discriminant = np.multiply( (X_.dot( w ) + b), y_ )\n",
    "    g = np.zeros( (B_eff,) )\n",
    "    g[discriminant < 1] = -1\n",
    "    delb = C * n/B_eff * g.dot( y_ )\n",
    "    delw = w + C * n/B_eff * (X_.T * g).dot( y_ )\n",
    "    return np.append( delw, delb )\n",
    "\n",
    "# Get the CSVM objective value in order to plot convergence curves\n",
    "def getLassoObjVal( theta ):\n",
    "    w = theta[0:-1]\n",
    "    b = theta[-1]\n",
    "    hingeLoss = np.maximum( 1 - np.multiply( (X.dot( w ) + b), y ), 0 )\n",
    "    return 0.5 * w.dot( w ) + C * np.sum( hingeLoss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7ed72e97b00c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mobjFun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "objFun = np.linalg.norm(w,ord=1)+np.linalg.norm(X.dot(w)-y,ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-6-4c66e4e3ee3e>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-4c66e4e3ee3e>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    return (w, totTime)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "\t\tt = t + 1\n",
    "\t\tif t % spacing == 0:\n",
    "\t\t\ttoc = tm.perf_counter()\n",
    "\t\t\ttotTime = totTime + (toc - tic)\n",
    "\t\t\tif totTime > timeout:\n",
    "\t\t\t\treturn (w, totTime)\n",
    "\t\t\telse:\n",
    "\t\t\t\ttic = tm.perf_counter()\n",
    "                \n",
    "return (w, totTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MAE and LASSO objective on original data by translating the model\n",
    "def getLASSOObjNorm( model ):\n",
    "    w = model[:-1]\n",
    "    b = model[-1]\n",
    "    # Translate the model to work with original data features\n",
    "    b = b - w.dot(mu / sg)\n",
    "    w = w / sg\n",
    "    res = X_extend.dot(w) + b - y\n",
    "    objVal = alpha * nplin.norm( w, 1 ) + 1/(2*n) * ( nplin.norm( res ) ** 2 )\n",
    "    MAEVal = np.mean( np.abs( res ) )\n",
    "    return (objVal, MAEVal)\n",
    "\n",
    "# Get the gradient to the loss function in LASSO for normalized data\n",
    "def getLASSOGradNorm( model, t ):\n",
    "    w = model[:-1]\n",
    "    b = model[-1]\n",
    "    samples = random.sample( range(0, n), B )\n",
    "    X_ = XNorm[samples,:]\n",
    "    y_ = y[samples]\n",
    "    res = X_.dot(w) + b - y_\n",
    "    grad = np.append( X_.T.dot(res), np.sum(res) )\n",
    "    return grad/B\n",
    "\n",
    "# Set hyperparameters and initialize the model as before\n",
    "# Since our normalized data is better conditioned, we are able to use a much\n",
    "# bigger value of the step length parameter which leads to faster progress\n",
    "alpha = 1\n",
    "B = 10\n",
    "eta = 1e-2\n",
    "init = np.zeros( (d+1,) )\n",
    "modelPrev = np.zeros( (d+1,) )\n",
    "\n",
    "# A constant step length seems to work well here\n",
    "stepFunc = opt.stepLengthGenerator( \"constant\", eta )\n",
    "# Notice that we are running the ProxGD method for far fewer iterations (1000)\n",
    "# than we did (50000) when we had badly conditioned data\n",
    "(modelProxGD, objProxGD, timeProxGD) = opt.doGD( getLASSOGradNorm, stepFunc, getLASSOObjNorm, init, horizon = 1000, doModelAveraging = True, postGradFunc = doSoftThresholding )\n",
    "objVals = [objProxGD[i][0] for i in range(len(objProxGD))]\n",
    "MAEVals = [objProxGD[i][1] for i in range(len(objProxGD))]\n",
    "\n",
    "fig8 = pd.getFigure( 7, 7 )\n",
    "ax = plt.gca()\n",
    "ax.set_title( \"The Accelerated ProxGD Solver on Normalized Data\" )\n",
    "ax.set_xlabel( \"Elapsed time (sec)\" )\n",
    "ax.set_ylabel( \"Objective Value for LASSO\", color = \"r\" )\n",
    "ax.plot( timeProxGD, objVals, color = 'r', linestyle = ':' )\n",
    "ax2 = ax.twinx()\n",
    "ax2.set_ylabel( \"MAE Value for LASSO\", color = \"b\" )\n",
    "ax2.plot( timeProxGD, MAEVals, color = 'b', linestyle = '--' )\n",
    "plt.ylim( 2, 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxTop = np.argsort( np.abs(modelProxGD) )[::-1][:20]\n",
    "print( \"The top 20 coordinates in terms of magnitude are \\n \", idxTop )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
